[["index.html", "R Cheat Sheet Introduction About Setting up", " R Cheat Sheet Koen 2024-03-05 Introduction About This website is intended as a quick reference for some techiques that I think I may need when cleaning, analyzing, or presenting data. For a more basic intro to R try the R for Social Science Data Carpentry Workshop , on which some of this website is based. Setting up This book uses the SAFI data set, and a large number of libraries. The following will download them all. I use the here package to manage paths. #make sure your working folder is set to something sensible! file.create(&quot;.here&quot;) here::i_am(&quot;.here&quot;) library(here) dir.create(here(&quot;data&quot;)) download.file( &quot;https://raw.githubusercontent.com/datacarpentry/r-socialsci/main/episodes/data/SAFI_clean.csv&quot;, here(&quot;data/SAFI_clean.csv&quot;), mode = &quot;wb&quot; ) install.packages(c(&quot;tidyverse&quot;,&quot;here&quot;,&quot;arsenal&quot;,&quot;lmtest&quot;,&quot;sandwich&quot;,&quot;here&quot;, &quot;huxtable&quot;,&quot;flextable&quot;,&quot;declaredesign&quot;,&quot;fixest&quot;)) "],["datawrangling.html", "Data Wrangling Basic Data Manipulation Pivoting (or reshaping) Joining (or merging) data Summarizing over groups (or collapsing data) Row-wise Operations Splitting multi-response variable into dummies", " Data Wrangling Make sure you have the tidyverse installed, and the SAFI data set downloaded to your data folder by running the code from the Set-up section Basic Data Manipulation library(tidyverse) library(here) read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) %&gt;% filter(village == &quot;Chirodzo&quot;) %&gt;% mutate(people_per_room = no_membrs / rooms, day = day(interview_date), month = month(interview_date), year = year(interview_date)) %&gt;% select(key_ID:rooms, day:people_per_room, -village) %&gt;% filter(interview_date &gt; &quot;2016-12-1&quot; &amp; interview_date &lt; &#39;2017-01-01&#39;) ## # A tibble: 1 × 8 ## key_ID interview_date no_membrs years_liv respondent_wall_type rooms ## &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 10 2016-12-16 00:00:00 12 23 burntbricks 5 ## # ℹ 2 more variables: day &lt;int&gt;, people_per_room &lt;dbl&gt; Pivoting (or reshaping) In tidyverse, reshaping is called pivoting. Here’s how you pivot a household roster (reshape wider) so you can merge it with the household data. Creating fake data First, I create sa fake household roster, based on the SAFI data, making sure that the household roster has a number of lines for each household that is equal to the household size, and has two randomly generated variables: female and age. long_data &lt;- read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) %&gt;% select(key_ID,no_membrs ) %&gt;% uncount(no_membrs) %&gt;% group_by(key_ID) %&gt;% mutate(member_ID = row_number()) %&gt;% rowwise() %&gt;% mutate(female = sample(0:1,1), age = case_when(member_ID == 1 ~ sample(18:86,1), .default = sample(0:86,1))) %&gt;% ungroup() long_data ## # A tibble: 942 × 4 ## key_ID member_ID female age ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 58 ## 2 1 2 1 8 ## 3 1 3 1 3 ## 4 2 1 0 31 ## 5 2 2 1 72 ## 6 2 3 0 13 ## 7 2 4 1 21 ## 8 2 5 1 50 ## 9 2 6 1 70 ## 10 2 7 0 17 ## # ℹ 932 more rows Pivoting long to wide To merge this into our main data set, we need to make sure we go back to having 1 observation per household. We will do this by using pivot_wider(): wide_data &lt;- long_data %&gt;% pivot_wider(names_from = member_ID, values_from = !ends_with(&quot;_ID&quot;)) wide_data ## # A tibble: 131 × 39 ## key_ID female_1 female_2 female_3 female_4 female_5 female_6 female_7 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 1 NA NA NA NA ## 2 2 0 1 0 1 1 1 0 ## 3 3 0 1 1 1 0 0 0 ## 4 4 1 1 0 1 0 1 0 ## 5 5 1 0 1 0 0 0 1 ## 6 6 1 1 1 NA NA NA NA ## 7 7 1 0 0 0 1 0 NA ## 8 8 0 0 0 0 1 1 1 ## 9 9 1 0 1 0 1 1 0 ## 10 10 0 0 1 1 0 1 1 ## # ℹ 121 more rows ## # ℹ 31 more variables: female_8 &lt;int&gt;, female_9 &lt;int&gt;, female_10 &lt;int&gt;, ## # female_11 &lt;int&gt;, female_12 &lt;int&gt;, female_13 &lt;int&gt;, female_14 &lt;int&gt;, ## # female_15 &lt;int&gt;, female_16 &lt;int&gt;, female_17 &lt;int&gt;, female_18 &lt;int&gt;, ## # female_19 &lt;int&gt;, age_1 &lt;int&gt;, age_2 &lt;int&gt;, age_3 &lt;int&gt;, age_4 &lt;int&gt;, ## # age_5 &lt;int&gt;, age_6 &lt;int&gt;, age_7 &lt;int&gt;, age_8 &lt;int&gt;, age_9 &lt;int&gt;, ## # age_10 &lt;int&gt;, age_11 &lt;int&gt;, age_12 &lt;int&gt;, age_13 &lt;int&gt;, age_14 &lt;int&gt;, … We only needed to specify two options: names_from: this is the column that contains the names (or usually numbers) for each of our units of analysis. In this case, the member_ID. values_from: the variables containing the data. All variables you specify here, will get one column for each possible value of names_from. In our case, these variables female and age. I used tidy select syntax to specify all variables except the ones ending in “_ID”. Pivoting wide to long If we had started with wide data, and had wanted to transform to long data, we’d have to use pivot_longer(): recreated_long_data &lt;- wide_data %&gt;% pivot_longer(!key_ID, names_to = c(&quot;.value&quot;, &quot;member_ID&quot;), names_sep=&quot;_&quot;, values_drop_na = TRUE) long_data ## # A tibble: 942 × 4 ## key_ID member_ID female age ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 58 ## 2 1 2 1 8 ## 3 1 3 1 3 ## 4 2 1 0 31 ## 5 2 2 1 72 ## 6 2 3 0 13 ## 7 2 4 1 21 ## 8 2 5 1 50 ## 9 2 6 1 70 ## 10 2 7 0 17 ## # ℹ 932 more rows In this case, the syntax is a bit harder to understand than that of pivot wider. It’s good to think first what the original data looks like, and how I intend to transform it. The wide data has columns key_ID, age_1-19 and female_1-19. I don’t really want to touch the key_ID column. I want to turn the columns age_1-19 and female_1-19 into three columns: female, age and Member_ID, which contains the values 1-19. This translates to the options we passed to pivot_longer() as follows: !key_ID: We want to pivot the data that’s in all columns except key_ID. names_to = c(\".value\", \"member_ID\"): this specifies the new columns we want to create. It basically says that the existing column names consist of two parts: one part (i.e. female and age) that we wish to keep as column names , and one part (i.e. the numbers 1-19) which should be put into a new column which we will “member_ID”. \"names_sep=\": this indicates how the two parts mentioned above are separated. If there is no separator (for example your variables are called age1, age2, etc.) you’ll have to use thenames_pattern option. values_drop_na = TRUE: tells R to drop rows that have missing data for all variables. If we had set this to FALSE, we’d have 19 rows for each household, with a lot of missing data in all households smaller than 19 people. Joining (or merging) data Tidyverse has four functions to join (or merge, as Stata calls it) two data sets. The functions that differ in the way they treat observations that are in one data set but not the other. Consider the diagram below. It has two data sets, x (in Stata terms, this is the master data set) and y (the using data set in Stata terms). They have overlapping rows (area B), but also rows that are only in x (area A) or only in y (area C). The four join functions work as follows: inner_join() will only keep area B. left_join() will keep areas A and B. right_join() will keep areas B and C. full_join() will keep areas A, B, and C. In our case, the data sets match perfectly, i.e. we only have an area B, so there is no practical difference. I chose left_join() so the number of observations in my household survey is guaranteed to remain the same. To merge the roster to the household data, we use the join_by function: read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) %&gt;% left_join(wide_data) ## Rows: 131 Columns: 14 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (7): village, respondent_wall_type, memb_assoc, affect_conflicts, items... ## dbl (6): key_ID, no_membrs, years_liv, rooms, liv_count, no_meals ## dttm (1): interview_date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Joining with `by = join_by(key_ID)` ## # A tibble: 131 × 52 ## key_ID village interview_date no_membrs years_liv respondent_wall_type ## &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 God 2016-11-17 00:00:00 3 4 muddaub ## 2 2 God 2016-11-17 00:00:00 7 9 muddaub ## 3 3 God 2016-11-17 00:00:00 10 15 burntbricks ## 4 4 God 2016-11-17 00:00:00 7 6 burntbricks ## 5 5 God 2016-11-17 00:00:00 7 40 burntbricks ## 6 6 God 2016-11-17 00:00:00 3 3 muddaub ## 7 7 God 2016-11-17 00:00:00 6 38 muddaub ## 8 8 Chirodzo 2016-11-16 00:00:00 12 70 burntbricks ## 9 9 Chirodzo 2016-11-16 00:00:00 8 6 burntbricks ## 10 10 Chirodzo 2016-12-16 00:00:00 12 23 burntbricks ## # ℹ 121 more rows ## # ℹ 46 more variables: rooms &lt;dbl&gt;, memb_assoc &lt;chr&gt;, affect_conflicts &lt;chr&gt;, ## # liv_count &lt;dbl&gt;, items_owned &lt;chr&gt;, no_meals &lt;dbl&gt;, months_lack_food &lt;chr&gt;, ## # instanceID &lt;chr&gt;, female_1 &lt;int&gt;, female_2 &lt;int&gt;, female_3 &lt;int&gt;, ## # female_4 &lt;int&gt;, female_5 &lt;int&gt;, female_6 &lt;int&gt;, female_7 &lt;int&gt;, ## # female_8 &lt;int&gt;, female_9 &lt;int&gt;, female_10 &lt;int&gt;, female_11 &lt;int&gt;, ## # female_12 &lt;int&gt;, female_13 &lt;int&gt;, female_14 &lt;int&gt;, female_15 &lt;int&gt;, … Note that we didn’t specify identifiers, like we would in Stata. R assumed that the variables that appear in both data frames are the identifiers, in this case key_ID. Use the by option to change this. Going the other way around, joining the household data to the roster data, is equally easy: long_data %&gt;% left_join( read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) %&gt;% select(key_ID,village,interview_date)) ## Rows: 131 Columns: 14 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (7): village, respondent_wall_type, memb_assoc, affect_conflicts, items... ## dbl (6): key_ID, no_membrs, years_liv, rooms, liv_count, no_meals ## dttm (1): interview_date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Joining with `by = join_by(key_ID)` ## # A tibble: 942 × 6 ## key_ID member_ID female age village interview_date ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dttm&gt; ## 1 1 1 1 58 God 2016-11-17 00:00:00 ## 2 1 2 1 8 God 2016-11-17 00:00:00 ## 3 1 3 1 3 God 2016-11-17 00:00:00 ## 4 2 1 0 31 God 2016-11-17 00:00:00 ## 5 2 2 1 72 God 2016-11-17 00:00:00 ## 6 2 3 0 13 God 2016-11-17 00:00:00 ## 7 2 4 1 21 God 2016-11-17 00:00:00 ## 8 2 5 1 50 God 2016-11-17 00:00:00 ## 9 2 6 1 70 God 2016-11-17 00:00:00 ## 10 2 7 0 17 God 2016-11-17 00:00:00 ## # ℹ 932 more rows Note that here I only merged in two variables, by using select and a pipe within the left_join() function. Summarizing over groups (or collapsing data) To compute summary statistics (sums, counts, means etc.) over a group, we use the group_by() and summarize() functions. For example, to compute the household size, number of women and average age in each household: long_data %&gt;% group_by(key_ID) %&gt;% summarize(hh_size = n(), num_women = sum(female), mean_age = mean(age)) ## # A tibble: 131 × 4 ## key_ID hh_size num_women mean_age ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 3 3 23 ## 2 2 7 4 39.1 ## 3 3 10 4 37.1 ## 4 4 7 4 50 ## 5 5 7 3 42.9 ## 6 6 3 3 63.7 ## 7 7 6 2 40.3 ## 8 8 12 6 51.2 ## 9 9 8 4 48.2 ## 10 10 12 7 51.2 ## # ℹ 121 more rows Row-wise Operations Suppose we wanted to run an operation over multiple variables. For example to get the household size, number of women and average age. The easiest, and probably best, way to do this in R is by reshaping to long, and the use summarize, like we did above. But in Stata you would probably use some sort of egen function, so that may come natural. You can do similar things in R. It’s just a bit more complex than in Stata: wide_data %&gt;% rowwise() %&gt;% mutate(mean_age = mean(c_across(starts_with(&quot;age_&quot;)), na.rm=TRUE), num_women = sum(c_across(starts_with(&quot;female_&quot;)), na.rm=TRUE), hh_size = sum(!is.na(c_across(starts_with(&quot;female_&quot;))), na.rm=TRUE)) %&gt;% select(key_ID,hh_size,num_women,mean_age) %&gt;% ungroup() ## # A tibble: 131 × 4 ## key_ID hh_size num_women mean_age ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 3 3 23 ## 2 2 7 4 39.1 ## 3 3 10 4 37.1 ## 4 4 7 4 50 ## 5 5 7 3 42.9 ## 6 6 3 3 63.7 ## 7 7 6 2 40.3 ## 8 8 12 6 51.2 ## 9 9 8 4 48.2 ## 10 10 12 7 51.2 ## # ℹ 121 more rows The key trick here is the combination of rowwise() and c_across(). rowwise() ensure all summaries are computed per row, and c_across() allows you to use tidy select syntax within the mean() and sum() functions. sum(!is.na()) simply counts the non-missing values. Splitting multi-response variable into dummies The SAFI data contains a number of columns that contain all responses selected in a multiple response questions. For example, the variables items_owned can contain something like \"bicycle;television;solar_panel;table\". We want to split this into dummies: one for each possible answers. There’s a number of ways to do this: First, you can use str_count() for each possible answer: split_without_loop &lt;- read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) %&gt;% select(items_owned) %&gt;% mutate(owns_bicycle = str_count(.data$items_owned,&quot;bicycle&quot;), owns_television = str_count(.data$items_owned,&quot;television&quot;)) #etc etc. split_without_loop ## # A tibble: 131 × 3 ## items_owned owns_bicycle owns_television ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 bicycle;television;solar_panel;table 1 1 ## 2 cow_cart;bicycle;radio;cow_plough;solar_panel;s… 1 0 ## 3 solar_torch 0 0 ## 4 bicycle;radio;cow_plough;solar_panel;mobile_pho… 1 0 ## 5 motorcyle;radio;cow_plough;mobile_phone 0 0 ## 6 &lt;NA&gt; NA NA ## 7 motorcyle;cow_plough 0 0 ## 8 motorcyle;bicycle;television;radio;cow_plough;s… 1 1 ## 9 television;solar_panel;solar_torch 0 1 ## 10 cow_cart;motorcyle;bicycle;television;radio;cow… 1 1 ## # ℹ 121 more rows Using loops This gets extremely tedious, better to put everything in a loop: #define the vector to loop over items &lt;- c(&quot;bicycle&quot;,&quot;television&quot;,&quot;solar_panel&quot;,&quot;table&quot;,&quot;cow_cart&quot;,&quot;radio&quot;, &quot;cow_plough&quot;,&quot;solar_torch&quot;,&quot;mobile_phone&quot;,&quot;motor_cycle&quot;,&quot;fridge&quot;, &quot;electricity&quot;,&quot;sofa_set&quot;,&quot;lorry&quot;,&quot;sterio&quot;,&quot;computer&quot;,&quot;car&quot;) #prepare the data split_with_loop &lt;- read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) %&gt;% select(items_owned) #and add to the data each iteration of a loop for (i in items){ split_with_loop &lt;- split_with_loop %&gt;% mutate(&quot;owns_{i}&quot; := str_count(.data$items_owned,i)) } split_with_loop ## # A tibble: 131 × 18 ## items_owned owns_bicycle owns_television owns_solar_panel owns_table ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 bicycle;television;… 1 1 1 1 ## 2 cow_cart;bicycle;ra… 1 0 1 1 ## 3 solar_torch 0 0 0 0 ## 4 bicycle;radio;cow_p… 1 0 1 0 ## 5 motorcyle;radio;cow… 0 0 0 0 ## 6 &lt;NA&gt; NA NA NA NA ## 7 motorcyle;cow_plough 0 0 0 0 ## 8 motorcyle;bicycle;t… 1 1 1 1 ## 9 television;solar_pa… 0 1 1 0 ## 10 cow_cart;motorcyle;… 1 1 1 1 ## # ℹ 121 more rows ## # ℹ 13 more variables: owns_cow_cart &lt;int&gt;, owns_radio &lt;int&gt;, ## # owns_cow_plough &lt;int&gt;, owns_solar_torch &lt;int&gt;, owns_mobile_phone &lt;int&gt;, ## # owns_motor_cycle &lt;int&gt;, owns_fridge &lt;int&gt;, owns_electricity &lt;int&gt;, ## # owns_sofa_set &lt;int&gt;, owns_lorry &lt;int&gt;, owns_sterio &lt;int&gt;, ## # owns_computer &lt;int&gt;, owns_car &lt;int&gt; Note how we used \"owns_{i}\" := to dynamically create variable names in the mutate() function. Using map() and pmap() Loops have the reputation of being slow, so if performance is important you can also use map() to repeat a function for every element in a vector. It returns a list where each list item is the output of one iteration (i.e. one new column): split_with_map &lt;- read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) items %&gt;% map(\\(x) str_count(split_with_map$items_owned,x))%&gt;% head(n = 2) ## [[1]] ## [1] 1 1 0 1 0 NA 0 1 0 1 0 1 1 1 1 0 0 1 1 1 NA 0 1 0 0 ## [26] 0 1 NA 1 1 NA 0 0 0 1 1 1 1 NA 1 1 0 0 0 1 0 0 0 1 0 ## [51] 0 0 1 NA 0 1 0 1 NA 0 1 1 NA 1 0 1 0 0 1 1 0 0 1 1 NA ## [76] 0 0 0 0 1 0 0 1 1 0 1 NA 1 0 1 1 1 0 0 1 1 0 0 0 1 ## [101] 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 ## [126] 1 0 0 0 0 0 ## ## [[2]] ## [1] 1 0 0 0 0 NA 0 1 1 1 0 0 0 0 0 0 0 0 0 0 NA 0 1 0 1 ## [26] 0 0 NA 0 0 NA 0 0 1 0 0 1 0 NA 0 0 0 0 0 1 1 0 0 0 0 ## [51] 0 1 0 NA 1 0 0 1 NA 0 1 0 NA 0 0 1 0 1 0 0 0 0 1 0 NA ## [76] 0 0 1 0 1 0 0 1 0 0 1 NA 0 0 0 1 0 0 0 0 0 0 0 0 0 ## [101] 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 ## [126] 1 0 1 0 1 0 A list isn’t really great in this case. map_df() returns its items as columns in a dataframe. For this to work, I make sure the vector has names, which will then become the column names: items %&gt;% set_names(paste0(&quot;owns_&quot;,items)) %&gt;% map_df(\\(x) str_count(split_with_map$items_owned,x)) ## # A tibble: 131 × 17 ## owns_bicycle owns_television owns_solar_panel owns_table owns_cow_cart ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 1 0 ## 2 1 0 1 1 1 ## 3 0 0 0 0 0 ## 4 1 0 1 0 0 ## 5 0 0 0 0 0 ## 6 NA NA NA NA NA ## 7 0 0 0 0 0 ## 8 1 1 1 1 0 ## 9 0 1 1 0 0 ## 10 1 1 1 1 1 ## # ℹ 121 more rows ## # ℹ 12 more variables: owns_radio &lt;int&gt;, owns_cow_plough &lt;int&gt;, ## # owns_solar_torch &lt;int&gt;, owns_mobile_phone &lt;int&gt;, owns_motor_cycle &lt;int&gt;, ## # owns_fridge &lt;int&gt;, owns_electricity &lt;int&gt;, owns_sofa_set &lt;int&gt;, ## # owns_lorry &lt;int&gt;, owns_sterio &lt;int&gt;, owns_computer &lt;int&gt;, owns_car &lt;int&gt; One problem is our pipe started with items, making it a bit more difficult to use this is a data management pipeline. To solve this, you can also iterate over the rows in a data frame using pmap(), so that each element in our list is a row: read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) %&gt;% pmap(\\(items_owned, ...) str_count(items_owned,items)) %&gt;% head(n = 2) ## [[1]] ## [1] 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ## ## [[2]] ## [1] 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 Note that pmap passes all variables in the data frame as arguments to the function. Here, I only need items_owned, so I capture all unneeded variables with .... To bind this together to a data frame, I need to make sure that the output of each iteration of pmap() is a row with as_tibble_row(), using its .name_repair option to assing names in the same way I did to the items vector above. read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) %&gt;% pmap_dfr(\\(items_owned, ...) str_count(items_owned,items) %&gt;% as_tibble_row(.name_repair = ~paste0(&quot;owns_&quot;,items))) ## # A tibble: 131 × 17 ## owns_bicycle owns_television owns_solar_panel owns_table owns_cow_cart ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 1 0 ## 2 1 0 1 1 1 ## 3 0 0 0 0 0 ## 4 1 0 1 0 0 ## 5 0 0 0 0 0 ## 6 NA NA NA NA NA ## 7 0 0 0 0 0 ## 8 1 1 1 1 0 ## 9 0 1 1 0 0 ## 10 1 1 1 1 1 ## # ℹ 121 more rows ## # ℹ 12 more variables: owns_radio &lt;int&gt;, owns_cow_plough &lt;int&gt;, ## # owns_solar_torch &lt;int&gt;, owns_mobile_phone &lt;int&gt;, owns_motor_cycle &lt;int&gt;, ## # owns_fridge &lt;int&gt;, owns_electricity &lt;int&gt;, owns_sofa_set &lt;int&gt;, ## # owns_lorry &lt;int&gt;, owns_sterio &lt;int&gt;, owns_computer &lt;int&gt;, owns_car &lt;int&gt; Using rowwise(), mutate() and unnest() While map() is faster, you can use mutate() to nest a data set into a variable. That, combined with as_tibble_row(): read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) %&gt;% rowwise() %&gt;% mutate(x = as_tibble_row(str_count(items_owned,items), .name_repair = ~paste0(&quot;owns_&quot;, items))) %&gt;% unnest(cols=c(x)) ## # A tibble: 131 × 31 ## key_ID village interview_date no_membrs years_liv respondent_wall_type ## &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 God 2016-11-17 00:00:00 3 4 muddaub ## 2 2 God 2016-11-17 00:00:00 7 9 muddaub ## 3 3 God 2016-11-17 00:00:00 10 15 burntbricks ## 4 4 God 2016-11-17 00:00:00 7 6 burntbricks ## 5 5 God 2016-11-17 00:00:00 7 40 burntbricks ## 6 6 God 2016-11-17 00:00:00 3 3 muddaub ## 7 7 God 2016-11-17 00:00:00 6 38 muddaub ## 8 8 Chirodzo 2016-11-16 00:00:00 12 70 burntbricks ## 9 9 Chirodzo 2016-11-16 00:00:00 8 6 burntbricks ## 10 10 Chirodzo 2016-12-16 00:00:00 12 23 burntbricks ## # ℹ 121 more rows ## # ℹ 25 more variables: rooms &lt;dbl&gt;, memb_assoc &lt;chr&gt;, affect_conflicts &lt;chr&gt;, ## # liv_count &lt;dbl&gt;, items_owned &lt;chr&gt;, no_meals &lt;dbl&gt;, months_lack_food &lt;chr&gt;, ## # instanceID &lt;chr&gt;, owns_bicycle &lt;int&gt;, owns_television &lt;int&gt;, ## # owns_solar_panel &lt;int&gt;, owns_table &lt;int&gt;, owns_cow_cart &lt;int&gt;, ## # owns_radio &lt;int&gt;, owns_cow_plough &lt;int&gt;, owns_solar_torch &lt;int&gt;, ## # owns_mobile_phone &lt;int&gt;, owns_motor_cycle &lt;int&gt;, owns_fridge &lt;int&gt;, … Using separate_longer() If writing a vector with possible values is too tedious/error prone/impossible: read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) %&gt;% separate_longer_delim(items_owned, delim = &quot;;&quot;) %&gt;% mutate(value = 1) %&gt;% pivot_wider(names_from = items_owned, values_from = value, names_glue = &quot;owns_{items_owned}&quot;, values_fill = 0) ## # A tibble: 131 × 31 ## key_ID village interview_date no_membrs years_liv respondent_wall_type ## &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 God 2016-11-17 00:00:00 3 4 muddaub ## 2 2 God 2016-11-17 00:00:00 7 9 muddaub ## 3 3 God 2016-11-17 00:00:00 10 15 burntbricks ## 4 4 God 2016-11-17 00:00:00 7 6 burntbricks ## 5 5 God 2016-11-17 00:00:00 7 40 burntbricks ## 6 6 God 2016-11-17 00:00:00 3 3 muddaub ## 7 7 God 2016-11-17 00:00:00 6 38 muddaub ## 8 8 Chirodzo 2016-11-16 00:00:00 12 70 burntbricks ## 9 9 Chirodzo 2016-11-16 00:00:00 8 6 burntbricks ## 10 10 Chirodzo 2016-12-16 00:00:00 12 23 burntbricks ## # ℹ 121 more rows ## # ℹ 25 more variables: rooms &lt;dbl&gt;, memb_assoc &lt;chr&gt;, affect_conflicts &lt;chr&gt;, ## # liv_count &lt;dbl&gt;, no_meals &lt;dbl&gt;, months_lack_food &lt;chr&gt;, instanceID &lt;chr&gt;, ## # owns_bicycle &lt;dbl&gt;, owns_television &lt;dbl&gt;, owns_solar_panel &lt;dbl&gt;, ## # owns_table &lt;dbl&gt;, owns_cow_cart &lt;dbl&gt;, owns_radio &lt;dbl&gt;, ## # owns_cow_plough &lt;dbl&gt;, owns_solar_torch &lt;dbl&gt;, owns_mobile_phone &lt;dbl&gt;, ## # owns_motorcyle &lt;dbl&gt;, owns_NA &lt;dbl&gt;, owns_fridge &lt;dbl&gt;, … Note that this solution differs in how observations where items_owned is empty are treated. Previous solutions had NA for all created variables in this case; using this approach, a variable owns_NA is created, with value 1, and all other variables are set to 0. It’s unclear which is better in this case: does an empty items_owned means the household owns nothing? Then this implementation is better. Does it mean the quesitons was skipped? Then the previous ones were better. "],["reporting.html", "Estimating and reporting Generating some fake data Making a table of summary statistics Simple regression Robust standard errors Exporting to word", " Estimating and reporting This chapter uses a large number of packages, and the SAFI data set, so make sure all are downloaded by running the code from the Set-up section. I will create a table of descriptive statistics, and a simple regression table. Generating some fake data First we make a fake intervention aimed at improving fertilizer adoption. Adoption depends on the treatment and education and a random component. The page on DeclareDesign has more advanced techniques to generate fake data. library(tidyverse) library(here) rm(list=ls()) set.seed(1) data &lt;- read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) %&gt;% left_join({.} %&gt;% select(village) %&gt;% distinct(village) %&gt;% rowwise %&gt;% mutate(treatment = rbinom(1,1,0.5)))%&gt;% rowwise() %&gt;% mutate(educated = rbinom(1,1,0.3), u = sample(c(0.1,0.2,0.3),1), prob = 0.3 * treatment + 0.1 * educated + u, uses_fertilizer = rbinom(1,1,prob)) %&gt;% ungroup() %&gt;% select(-prob,-u) ## Rows: 131 Columns: 14 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (7): village, respondent_wall_type, memb_assoc, affect_conflicts, items... ## dbl (6): key_ID, no_membrs, years_liv, rooms, liv_count, no_meals ## dttm (1): interview_date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Joining with `by = join_by(village)` Making a table of summary statistics Using Arsenal The first table we make is a table of descriptive statistics using the Arsenal package: library(arsenal) descriptive_table &lt;- data %&gt;% select(where(is.numeric),-key_ID) %&gt;% tableby(treatment ~ ., data = .) summary(descriptive_table) ## ## ## | | 0 (N=82) | 1 (N=49) | Total (N=131) | p value| ## |:---------------------------|:---------------:|:---------------:|:---------------:|-------:| ## |**no_membrs** | | | | 0.290| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 6.963 (2.856) | 7.571 (3.640) | 7.191 (3.172) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 2.000 - 15.000 | 2.000 - 19.000 | 2.000 - 19.000 | | ## |**years_liv** | | | | 0.331| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 21.939 (14.921) | 24.918 (19.832) | 23.053 (16.913) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 1.000 - 70.000 | 2.000 - 96.000 | 1.000 - 96.000 | | ## |**rooms** | | | | 0.963| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 1.744 (1.174) | 1.735 (0.953) | 1.740 (1.093) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 1.000 - 8.000 | 1.000 - 4.000 | 1.000 - 8.000 | | ## |**liv_count** | | | | 0.094| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 2.244 (1.013) | 2.571 (1.173) | 2.366 (1.083) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 1.000 - 4.000 | 1.000 - 5.000 | 1.000 - 5.000 | | ## |**no_meals** | | | | 0.596| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 2.585 (0.496) | 2.633 (0.487) | 2.603 (0.491) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 2.000 - 3.000 | 2.000 - 3.000 | 2.000 - 3.000 | | ## |**educated** | | | | 0.872| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 0.293 (0.458) | 0.306 (0.466) | 0.298 (0.459) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 0.000 - 1.000 | 0.000 - 1.000 | 0.000 - 1.000 | | ## |**uses_fertilizer** | | | | 0.002| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 0.244 (0.432) | 0.510 (0.505) | 0.344 (0.477) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 0.000 - 1.000 | 0.000 - 1.000 | 0.000 - 1.000 | | That summary looks okay, even as raw text, and does what we want, so let’s export it to word! For this, we use Arsenal’s write2word() function. I had some trouble getting it to pick up my path, so I am using the here library to generate a path for me. This library is a must for shared projects! write2word(descriptive_table, here(&quot;tables/summ_stats_arsenal.docx&quot;), title = &quot;Descriptive Statistics&quot;, quiet = TRUE) You can find the resulting .docx here. Using Flextable For more control, the flextable package can covert data frames into good-looking table using the tabulator() function. First, make a data frame with summarry statistics. I duplicate the data set using bind_rows() to create an overall group. summstats &lt;- bind_rows(data %&gt;% mutate(Treatment = ifelse(treatment,&quot; Treatment&quot;,&quot; Control&quot;)), data %&gt;% mutate(Treatment = &quot;Overall&quot;)) %&gt;% select(where(is.numeric),Treatment,-key_ID,-treatment) %&gt;% group_by(Treatment) %&gt;% summarize(across(everything(), list(n = ~sum(!is.na(.x)), nmiss = ~sum(is.na(.x)), mean = ~mean(.x,na.rm=TRUE), sd = ~sd(.x,na.rm=TRUE), min = ~min(.x,na.rm=TRUE), max = ~max(.x,na.rm=TRUE), iqr = ~IQR(.x,na.rm=TRUE)), .names = &quot;{.col}-{.fn}&quot;)) %&gt;% pivot_longer(cols = -Treatment, names_to = c(&quot;Variable&quot;,&quot;.value&quot;), names_sep=&quot;-&quot;) summstats ## # A tibble: 21 × 9 ## Treatment Variable n nmiss mean sd min max iqr ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot; Control&quot; no_membrs 82 0 6.96 2.86 2 15 3.75 ## 2 &quot; Control&quot; years_liv 82 0 21.9 14.9 1 70 13.8 ## 3 &quot; Control&quot; rooms 82 0 1.74 1.17 1 8 1 ## 4 &quot; Control&quot; liv_count 82 0 2.24 1.01 1 4 2 ## 5 &quot; Control&quot; no_meals 82 0 2.59 0.496 2 3 1 ## 6 &quot; Control&quot; educated 82 0 0.293 0.458 0 1 1 ## 7 &quot; Control&quot; uses_fertilizer 82 0 0.244 0.432 0 1 0 ## 8 &quot; Treatment&quot; no_membrs 49 0 7.57 3.64 2 19 5 ## 9 &quot; Treatment&quot; years_liv 49 0 24.9 19.8 2 96 22 ## 10 &quot; Treatment&quot; rooms 49 0 1.73 0.953 1 4 1 ## # ℹ 11 more rows Then use I flextable’s tabulator() to make output that looks good in word. Note that tabulator() sorts the columns alphabetically, so that would be control, overall, treatment. That doesn’t make sense, so I have used spaces (\" Treatment\") to control the ordering. I’ve added a bunch of statistics to show the flexibility: library(flextable) summstats %&gt;% tabulator(rows = &quot;Variable&quot;, columns = &quot;Treatment&quot;, `N` = as_paragraph(as_chunk(n,digits=0)), `Mean (SD)` = as_paragraph(as_chunk(fmt_avg_dev(mean, sd, digit1=2,digit2 = 2))), Range = as_paragraph(as_chunk(min), &quot;-&quot;,as_chunk(max)) ) %&gt;% as_flextable() .cl-2995afa6{}.cl-298de0be{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-2990aca4{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2990acae{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1;background-color:transparent;}.cl-2990acb8{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2990acb9{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1;background-color:transparent;}.cl-2990acc2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2990acc3{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1;background-color:transparent;}.cl-2990acc4{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2990c4dc{width:1.325in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c4e6{width:0.079in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c4e7{width:0.458in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c4f0{width:1.306in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c4fa{width:0.902in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c4fb{width:0.556in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c504{width:1.325in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c505{width:0.079in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c50e{width:0.458in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c50f{width:1.306in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c510{width:0.902in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c511{width:0.556in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c518{width:1.325in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c519{width:0.079in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c51a{width:0.458in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c522{width:1.306in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c523{width:0.902in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c52c{width:0.556in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c52d{width:1.325in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c536{width:0.079in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c537{width:0.458in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c538{width:1.306in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c540{width:0.902in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c541{width:0.556in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c542{width:1.325in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c54a{width:0.458in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c54b{width:1.306in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c554{width:0.902in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2990c555{width:0.556in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}Variable Control TreatmentOverallNMean (SD)RangeNMean (SD)RangeNMean (SD)Rangeeducated820.29 (0.46)0.0-1.0490.31 (0.47)0.0-1.01310.30 (0.46)0.0-1.0liv_count822.24 (1.01)1.0-4.0492.57 (1.17)1.0-5.01312.37 (1.08)1.0-5.0no_meals822.59 (0.50)2.0-3.0492.63 (0.49)2.0-3.01312.60 (0.49)2.0-3.0no_membrs826.96 (2.86)2.0-15.0497.57 (3.64)2.0-19.01317.19 (3.17)2.0-19.0rooms821.74 (1.17)1.0-8.0491.73 (0.95)1.0-4.01311.74 (1.09)1.0-8.0uses_fertilizer820.24 (0.43)0.0-1.0490.51 (0.51)0.0-1.01310.34 (0.48)0.0-1.0years_liv8221.94 (14.92)1.0-70.04924.92 (19.83)2.0-96.013123.05 (16.91)1.0-96.0 To add a column with differences, I first define a function to compute the differences (I use a regression rather than a ttest, so I can cluster my standard errors etc. to this if I need to). Then I use summarize(across()) in much the same way as above, now to create a dataframe called difcol. Note that I use . to refer to the dataset currently in the %&gt;% pipe. get_diffs &lt;- function(.df,y,x){ reg &lt;- lm(y~ x) %&gt;% broom::tidy() coeff = round(reg[2,2],2) p &lt;- reg[2,5] stars = case_when(p &lt; 0.001 ~ &quot;***&quot;, p &lt; 0.01 ~ &quot;**&quot;, p &lt; 0.05 ~ &quot;*&quot;, .default = &quot;&quot; ) paste0(coeff,stars) } difcol &lt;- data %&gt;% select(where(is.numeric),-key_ID,-treatment) %&gt;% summarize(across(everything(), .fns = function(x) get_diffs(.,.$x,data$treatment))) %&gt;% pivot_longer(cols =everything(), names_to = &quot;Variable&quot;, values_to=&quot;Difference&quot;) Then, all I have to do is add it to tabulator() using its datasup_last argument. Below, I also use a few other flextable function to make the table nicer. In particular, labelizor() to add variable labels, which I define as a vector. #named vector with variable labels: labels &lt;- c(&quot;Household members&quot;,&quot;Years lived&quot;,&quot;Rooms&quot;,&quot;N. livestock owned&quot;,&quot;Meals per day&quot;, &quot;Educated&quot;,&quot;Uses Ferilizer&quot;) names(labels) &lt;- summstats[[2]][1:7] descriptive_table_flex &lt;- summstats %&gt;% tabulator(rows = &quot;Variable&quot;, columns = &quot;Treatment&quot;, datasup_last = difcol, `N` = as_paragraph(as_chunk(n,digits=0)), `Mean (SD)` = as_paragraph(as_chunk(fmt_avg_dev(mean, sd, digit1=2,digit2 = 2)))) %&gt;% as_flextable() %&gt;% labelizor(j = &quot;Variable&quot;, labels = labels, part = &quot;all&quot;) %&gt;% fix_border_issues() %&gt;% autofit() descriptive_table_flex .cl-29e4b1c8{}.cl-29dd73cc{font-family:'DejaVu Sans';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-29dfef94{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-29dfef9e{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1;background-color:transparent;}.cl-29dfefa8{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-29dfefb2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1;background-color:transparent;}.cl-29dfefb3{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-29dfefb4{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1;background-color:transparent;}.cl-29dfefbc{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-29e0039e{width:1.867in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003a8{width:0.1in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003b2{width:0.48in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003b3{width:1.328in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003bc{width:0.577in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003bd{width:1.078in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003c6{width:1.867in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003d0{width:0.1in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003d1{width:0.48in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003da{width:1.328in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003db{width:0.577in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003e4{width:1.078in;background-color:transparent;vertical-align: bottom;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003e5{width:1.867in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003e6{width:0.1in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003ee{width:0.48in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003ef{width:1.328in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003f0{width:0.577in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003f8{width:1.078in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003f9{width:1.867in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e003fa{width:0.1in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e00402{width:0.48in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e0040c{width:1.328in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e0040d{width:0.577in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e0040e{width:1.078in;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e00416{width:1.867in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e00417{width:0.48in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e00420{width:1.328in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e00421{width:0.577in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-29e0042a{width:1.078in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}Variable Control TreatmentOverallDifferenceNMean (SD)NMean (SD)NMean (SD)Educated820.29 (0.46)490.31 (0.47)1310.30 (0.46)0.01N. livestock owned822.24 (1.01)492.57 (1.17)1312.37 (1.08)0.33Meals per day822.59 (0.50)492.63 (0.49)1312.60 (0.49)0.05Household members826.96 (2.86)497.57 (3.64)1317.19 (3.17)0.61Rooms821.74 (1.17)491.73 (0.95)1311.74 (1.09)-0.01Uses Ferilizer820.24 (0.43)490.51 (0.51)1310.34 (0.48)0.27**Years lived8221.94 (14.92)4924.92 (19.83)13123.05 (16.91)2.98 To save it to word: descriptive_table_flex %&gt;% save_as_docx(path = here(&quot;tables/summs_stats_flex.docx&quot;)) You can find the resulting .docx here. Simple regression A simple regression uses the lm() function. We save the results in an object, which we can later include in a table we export to word. lm &lt;- lm(uses_fertilizer ~ treatment + educated, data = data) summary(lm) ## ## Call: ## lm(formula = uses_fertilizer ~ treatment + educated, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5987 -0.3341 -0.2066 0.5289 0.7934 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.20657 0.05677 3.639 0.000396 *** ## treatment 0.26459 0.08282 3.195 0.001762 ** ## educated 0.12757 0.08764 1.456 0.147949 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4586 on 128 degrees of freedom ## Multiple R-squared: 0.08871, Adjusted R-squared: 0.07447 ## F-statistic: 6.23 on 2 and 128 DF, p-value: 0.002618 Robust standard errors To get robust standard errors (bootstrapped) clustered at the village level, we can use the following: library(lmtest) library(sandwich) lm_robust &lt;- coeftest(lm, vcov = vcovBS(lm, cluster=~village)) lm_robust ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.206566 0.076970 2.6837 0.008244 ** ## treatment 0.264587 0.103139 2.5653 0.011460 * ## educated 0.127568 0.072263 1.7653 0.079895 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Let’s also do a probit model: probit &lt;- glm(uses_fertilizer ~ treatment + educated, family = binomial(link = &quot;probit&quot;), data = data) summary(probit) ## ## Call: ## glm(formula = uses_fertilizer ~ treatment + educated, family = binomial(link = &quot;probit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.8129 0.1731 -4.697 2.64e-06 *** ## treatment 0.7268 0.2359 3.081 0.00206 ** ## educated 0.3664 0.2499 1.467 0.14251 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 168.55 on 130 degrees of freedom ## Residual deviance: 156.85 on 128 degrees of freedom ## AIC: 162.85 ## ## Number of Fisher Scoring iterations: 4 And get robust SEs: probit_robust &lt;- coeftest(probit, vcov = vcovBS(probit,cluster=~village)) probit_robust ## ## z test of coefficients: ## ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.81293 0.27529 -2.9529 0.003148 ** ## treatment 0.72679 0.28738 2.5291 0.011436 * ## educated 0.36643 0.22746 1.6110 0.107179 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Exporting to word To make a good looking regression table, I use the huxtable library. Note the use of tidy_override() to add statistics to the model output. To export the resulting tables to word, I use the huxtable package. The package is extremely flexible, so check out its website. Some of the flexibility is seen below, by adding the number of clusters and formatting the number in the resulting table. #add the number of clusers to each lm_robust &lt;- huxtable::tidy_override(lm_robust, glance = list(n_clusters = 4), extend=TRUE) probit_robust &lt;- huxtable::tidy_override(probit_robust, glance = list(n_clusters = 4), extend=TRUE) #make the table reg_table &lt;- huxtable::huxreg(&quot;Linear&quot; = lm,&quot;Linear Robust&quot; = lm_robust, &quot;Probit&quot; = probit, &quot;Probit Robust&quot; = probit_robust, statistics = c(N = &quot;nobs&quot;, Clusters = &quot;n_clusters&quot;, &quot;Adj. R2&quot; = &quot;adj.r.squared&quot;)) %&gt;% huxtable::set_number_format(row=9,value=0) reg_table LinearLinear RobustProbitProbit Robust (Intercept)0.207 ***0.207 **-0.813 ***-0.813 ** (0.057)&nbsp;&nbsp;&nbsp;(0.077)&nbsp;&nbsp;(0.173)&nbsp;&nbsp;&nbsp;(0.275)&nbsp;&nbsp; treatment0.265 **&nbsp;0.265 *&nbsp;0.727 **&nbsp;0.727 *&nbsp; (0.083)&nbsp;&nbsp;&nbsp;(0.103)&nbsp;&nbsp;(0.236)&nbsp;&nbsp;&nbsp;(0.287)&nbsp;&nbsp; educated0.128&nbsp;&nbsp;&nbsp;&nbsp;0.128&nbsp;&nbsp;&nbsp;0.366&nbsp;&nbsp;&nbsp;&nbsp;0.366&nbsp;&nbsp;&nbsp; (0.088)&nbsp;&nbsp;&nbsp;(0.072)&nbsp;&nbsp;(0.250)&nbsp;&nbsp;&nbsp;(0.227)&nbsp;&nbsp; N131&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;131&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;131&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;131&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Clusters&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Adj. R20.074&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. That looks decent! However, huxtable doesn’t work well with Word. We can use as_flextable() to convert to flextable (note that I had bad results with flextable::as_flextable(), so make sure to use huxtable::as_flextable()), and then use flextable to export to docx. reg_table %&gt;% huxtable::as_flextable() %&gt;% flextable::save_as_docx(path = here(&quot;tables/regression_table.docx&quot;)) You can find there output here. "],["declaredesign.html", "Power Analysis using DeclareDesign Diff-in-Diff", " Power Analysis using DeclareDesign DeclareDesign is a system to simulate Research Designs. This is useful for power analysis, because it is often hard to include things like clustering and covariates in standard power calculators. Resources for learning about DeclareDesign: Slides by the authors of DeclareDesign: Graeme Blair, Alex Coppock, Macartan Humphreys The DeclareDesign CheatSheet The book Research Design in the Social Sciences: Declaration, Diagnosis, and Redesign Read at least the slides before going forward! Diff-in-Diff Here’s a common situation: I’ve collected some baseline data, and are wondering if our study has sufficient power to pick up expected treatment effects. I will start off with some fake baseline data, which only contains y0, our outcome indicator. We use the fabricate() function, from the fabricatr library, which is loaded with DeclareDesign: # start with a clean environment rm(list=ls()) # load packages library(tidyverse) # data management library(fixest) # for estimation of fixed effects library(lmtest) # for computing standard errors library(sandwich) # provides the functions for lmtest to compute robust SEs library(DeclareDesign) # for power calculation library(broom) # to extract coefficients from model output # set seed for reproducibility set.seed(1) N = 100 fake_data &lt;- fabricate(N = N, y0 = runif(N, 100, 150)) fake_data %&gt;% as_tibble() ## # A tibble: 100 × 2 ## ID y0 ## &lt;chr&gt; &lt;dbl&gt; ## 1 001 113. ## 2 002 119. ## 3 003 129. ## 4 004 145. ## 5 005 110. ## 6 006 145. ## 7 007 147. ## 8 008 133. ## 9 009 131. ## 10 010 103. ## # ℹ 90 more rows Now, let’s add an extra year to our data, again using fabricate(), just to demonstrate how, using add_level() and then crosslevels(): fakedata_years &lt;- fabricate(fake_data, years = add_level(N = 2, t = as.numeric(years) - 1, nest=FALSE), observations = cross_levels(by = join_using(ID, years))) fakedata_years %&gt;% as_tibble() ## # A tibble: 200 × 5 ## ID y0 years t observations ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 001 113. 1 0 001 ## 2 002 119. 1 0 002 ## 3 003 129. 1 0 003 ## 4 004 145. 1 0 004 ## 5 005 110. 1 0 005 ## 6 006 145. 1 0 006 ## 7 007 147. 1 0 007 ## 8 008 133. 1 0 008 ## 9 009 131. 1 0 009 ## 10 010 103. 1 0 010 ## # ℹ 190 more rows Now it’s time to start declaring our design. The first element of the design is the model, which essentially is my data, and I declare it using the same synytax as fabricate() above. This means you can input existing data, but also generate random new variables. The potential outcomes are generated using potential_outcomes(). This will create two variables Y_Z_1 and Y_Z_0, which are the potential outcomes if having received treatment (Z == 1) or not. Note that I will generate Z in the next step. You can see I have created a very simple data generating process to generate different outcomes in t = 1 for treatment and control, but you can go absolutely wild here to check the assumptions you have: you can cluster the outcomes, include compliance or treatment effectiveness, add more or less noise, make it depend on both observable and unobservable characteristics, etc. etc. year_shock &lt;- 10 effect_size &lt;- 15 stdev &lt;- 5 model &lt;- declare_model(fakedata_years, potential_outcomes(Y ~ y0 + t * year_shock + Z * effect_size + t * rnorm(N,sd = stdev))) Then it’s time to think about assignment. I need to assign treatment, and reveal the outcomes based on treatment assignment. Here I create two variables for treatment so I can toy around with different model specifications later on. The first variable is the treatment_group, to indicate which households get the treatment. For this I use the cluster_ra() because I have two observations per household, so households can be thought of as a cluster. The other is Z, an indicator for actually having received the treatment (equivalent to Z = treatment_group * t). If treatment had already been known during baseline (not unlikely) then we would have only had to reveal the outcomes: assignment &lt;- declare_assignment(treatment_group = cluster_ra(clusters = ID, prob = 0.5), Z = ifelse(treatment_group == 1 &amp; t == 1,1,0), Y = reveal_outcomes(Y ~ Z)) Next, I declare my theoretical quantity of interest is the treatment effect in year 1. This is useful to check for bias. This is not really needed here, since my model will follow our data generation process exactly, so I could have just skipped this step. inquiry &lt;- declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0), subset = (t == 1)) And finally, I add three estimators. If I hadn’t declared an inquiry, I could have declared a test here using declare_test(), which has the same syntax. The first estimator uses lm_robust(), which is included with DeclareDesign, to estimate a simple linear model using standard DiD notation. estimator1 &lt;- declare_estimator(Y ~ t * treatment_group, inquiry = &quot;ATE&quot;, .method = estimatr::lm_robust, term = &quot;t:treatment_group&quot;, label = &quot;LM&quot;) Then I use feols() from the fixest package to estimate a Two-way fixed effects (2FE) model. This is equivalent to the model above for this case, but the 2FE can be more easily extended to more complex designs (but beware!). library(fixest) library(broom) estimator2 &lt;- declare_estimator(Y ~ Z | treatment_group + t, vcov = &quot;hetero&quot;, .method = feols, term = &quot;Z&quot;, inquiry = &quot;ATE&quot;, label = &quot;2FE&quot;) As third estimator, I use a custom estimation function. The function takes and equation, the data to be analyzed, and (optionally) the type of standard errors to compute, and as its output it returns a tidy dataset containing estimated coefficients created by broom::tidy(). As long as you make sure the data can go into your custom function (using an argument called data), and it returns something that looks like what broom::tidy() would return, there’s no limit as to how complex you can make your analysis. Here, I keep it (relatively) simple. I use the lmtest/sandwich way of estimating robust standard errors, and make sure you can easily change the equation and type of Standard Errors to use through a parameter. library(lmtest) library(sandwich) custom_robust &lt;- function(equation, data, type=&quot;HC1&quot;){ # function takes a formula, provided as the first argument # of declare_estimator... # .. a type of standard errors to pass to vcov (default is HC1) # ... and data # It outputs a tidy data frame with relevant coefficients. lm(equation, data = data) %&gt;% coeftest(., vcov = vcovHC(., type=type)) %&gt;% broom::tidy(conf.int=TRUE) %&gt;% mutate(outcome = &quot;Y&quot;) %&gt;% filter(term == &quot;t:treatment_group&quot;) } # the arguments formula and type are passed to to custom_robust() # as is the data on which to run the estimator. estimator3 &lt;- declare_estimator(equation = Y ~ t * treatment_group, handler = label_estimator(custom_robust), inquiry = &quot;ATE&quot;, label = &quot;Custom&quot;) Finally, I combine all these elements to declare my design. Note that it’s only here that R starts actually running the code to randomize things. The previous was just declaration! design &lt;- model + assignment + inquiry + estimator1 + estimator2 + estimator3 summary(design) ## ## Research design declaration summary ## ## Step 1 (model): declare_model(fakedata_years, potential_outcomes(Y ~ y0 + t * year_shock + Z * effect_size + t * rnorm(N, sd = stdev))) ## ## N = 200 ## ## Added variable: ID ## N_missing N_unique class ## 0 100 character ## ## Added variable: y0 ## min median mean max sd N_missing N_unique ## 100.67 124.39 125.89 149.6 13.35 0 100 ## ## Added variable: years ## 1 2 ## 100 100 ## 0.50 0.50 ## ## Added variable: t ## 0 1 ## 100 100 ## 0.50 0.50 ## ## Added variable: observations ## N_missing N_unique class ## 0 200 character ## ## Added variable: Y_Z_0 ## min median mean max sd N_missing N_unique ## 100.67 132.43 130.95 160.63 14.16 0 200 ## ## Added variable: Y_Z_1 ## min median mean max sd N_missing N_unique ## 115.67 146.79 145.69 176.27 14.24 0 200 ## ## Step 2 (assignment): declare_assignment(treatment_group = cluster_ra(clusters = ID, prob = 0.5), Z = ifelse(treatment_group == 1 &amp; t == 1, 1, 0), Y = reveal_outcomes(Y ~ Z)) ## ## Added variable: treatment_group ## 0 1 ## 100 100 ## 0.50 0.50 ## ## Added variable: Z ## 0 1 ## 150 50 ## 0.75 0.25 ## ## Added variable: Y ## min median mean max sd N_missing N_unique ## 100.67 134.88 134.68 176.27 17.03 0 200 ## ## Step 3 (inquiry): declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0), subset = (t == 1)) ## ## A single draw of the inquiry: ## inquiry estimand ## ATE 14.47179 ## ## Step 4 (estimator): declare_estimator(Y ~ t * treatment_group, inquiry = &quot;ATE&quot;, .method = estimatr::lm_robust, term = &quot;t:treatment_group&quot;, label = &quot;LM&quot;) ## ## Formula: Y ~ t * treatment_group ## ## Method: :: Method: estimatr Method: lm_robust ## ## A single draw of the estimator: ## term estimator estimate std.error statistic p.value conf.low ## t:treatment_group LM 15.68729 3.867435 4.056252 7.194547e-05 8.060161 ## conf.high df outcome inquiry ## 23.31442 196 Y ATE ## ## Step 5 (estimator): declare_estimator(Y ~ Z | treatment_group + t, vcov = &quot;hetero&quot;, .method = feols, term = &quot;Z&quot;, inquiry = &quot;ATE&quot;, label = &quot;2FE&quot;) ## ## Formula: Y ~ Z | treatment_group + t ## ## Method: feols ## ## A single draw of the estimator: ## term estimator estimate std.error statistic p.value conf.low conf.high ## Z 2FE 15.68729 3.867435 4.056252 7.194547e-05 8.060161 23.31442 ## inquiry ## ATE ## ## Step 6 (estimator): declare_estimator(equation = Y ~ t * treatment_group, inquiry = &quot;ATE&quot;, handler = label_estimator(custom_robust), label = &quot;Custom&quot;) ## ## Formula: Y ~ t * treatment_group ## ## A single draw of the estimator: ## estimator term estimate std.error statistic p.value conf.low ## Custom t:treatment_group 15.68729 3.867435 4.056252 7.194547e-05 8.060161 ## conf.high outcome inquiry ## 23.31442 Y ATE Note that the results of my three estimators are identical, which is as expected. If you want to browse a version of the data created by your design, use the draw_data() function. This is useful to examine the properties of the data. draw_data(design) %&gt;% as_tibble() ## # A tibble: 200 × 10 ## ID y0 years t observations Y_Z_0 Y_Z_1 treatment_group Z Y ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 001 113. 1 0 001 113. 128. 1 0 113. ## 2 002 119. 1 0 002 119. 134. 1 0 119. ## 3 003 129. 1 0 003 129. 144. 0 0 129. ## 4 004 145. 1 0 004 145. 160. 0 0 145. ## 5 005 110. 1 0 005 110. 125. 0 0 110. ## 6 006 145. 1 0 006 145. 160. 1 0 145. ## 7 007 147. 1 0 007 147. 162. 1 0 147. ## 8 008 133. 1 0 008 133. 148. 0 0 133. ## 9 009 131. 1 0 009 131. 146. 1 0 131. ## 10 010 103. 1 0 010 103. 118. 0 0 103. ## # ℹ 190 more rows Now to calculate our power. The diagnose_design() will run our model 500 times and our power is simply the fraction of times we find a statistically significant effect. diagnose_design(design) ## ## Research design diagnosis based on 500 simulations. Diagnosis completed in 15 secs. Diagnosand estimates with bootstrapped standard errors in parentheses (100 replicates). ## ## Design Inquiry Estimator Outcome Term N Sims Mean Estimand ## design ATE 2FE &lt;NA&gt; Z 500 14.97 ## (0.03) ## design ATE Custom Y t:treatment_group 500 14.97 ## (0.03) ## design ATE LM Y t:treatment_group 500 14.97 ## (0.03) ## Mean Estimate Bias SD Estimate RMSE Power Coverage ## 15.02 0.05 1.04 0.70 1.00 1.00 ## (0.05) (0.04) (0.03) (0.02) (0.00) (0.00) ## 15.02 0.05 1.04 0.70 1.00 1.00 ## (0.05) (0.04) (0.03) (0.02) (0.00) (0.00) ## 15.02 0.05 1.04 0.70 1.00 1.00 ## (0.05) (0.04) (0.03) (0.02) (0.00) (0.00) Looks good! (Except for some NAs for the 2FE model: this is due to the summary function not providing all things needed. To fix this, I could have created my own summary or handler function, but it doesn’t really affect anything, so this is good enough.) But what if I was too optimistic? Using the redesign() function, you can vary various parameters of your design, and test all their combinations. Here I check a few plausible values for the stdev variable (giving the variation of the error term for incomes in year 1) and the expected effect size. Note that by default, this runs all estimators, 500 times for each combination of the parameters. This takes a lot of time, so I update my design to have only one estimator (estimator1, which is fast) and I’ve set the sims option of diagnose_design() to 200. design &lt;- model + assignment + inquiry + estimator1 diagnosis &lt;- design %&gt;% redesign(effect_size = 6:12, stdev = c(3,5,7)) %&gt;% diagnose_design(sims = 200) Now, I want a nice plot. For this I use the tidy() function to convert the output to a data frame, which - after some filtering - I pipe into ggplot(). It’s now easy to see that I should be able to pick up an effect size of 10 in most cases. diagnosis %&gt;% tidy() %&gt;% filter(diagnosand == &quot;power&quot;) %&gt;% select(effect_size,stdev,power = estimate) %&gt;% mutate(stdev = factor(stdev)) %&gt;% ggplot(aes(x = effect_size, y = power, shape = stdev, color=stdev)) + geom_line() + geom_point() + geom_hline(yintercept=0.8,linetype=2) + scale_x_continuous(breaks=seq(6,12,2)) "],["functions.html", "Functions Basic structure Using functions in a pipe Passing variables to a function Function to create dummies from text", " Functions Functions are great to re-use and structure your code. Basic structure add_two &lt;- function(argument) { # Do something with the input return &lt;- argument + 2 #call the intended output in the last line return } add_two(84) ## [1] 86 Using functions in a pipe By default, a function in a pipe uses the dataset of the pipe as its first argument. Also make sure to include a quick description of what your function does. library(tidyverse) add_number_column &lt;- function(data,number=2) { #This functions takes a data frame, and returns the same data frame, #but with an exta column of 2sas its first variable. #The user can specify a number other than 2 using the num argument #This is great if you want a column of twos as the first variable #of your data set. data %&gt;% mutate(bestvar = number) %&gt;% select(bestvar,everything()) } read_csv(here(&quot;data/SAFI_clean.csv&quot;),na = &quot;NULL&quot;) %&gt;% add_number_column() ## # A tibble: 131 × 15 ## bestvar key_ID village interview_date no_membrs years_liv ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 1 God 2016-11-17 00:00:00 3 4 ## 2 2 2 God 2016-11-17 00:00:00 7 9 ## 3 2 3 God 2016-11-17 00:00:00 10 15 ## 4 2 4 God 2016-11-17 00:00:00 7 6 ## 5 2 5 God 2016-11-17 00:00:00 7 40 ## 6 2 6 God 2016-11-17 00:00:00 3 3 ## 7 2 7 God 2016-11-17 00:00:00 6 38 ## 8 2 8 Chirodzo 2016-11-16 00:00:00 12 70 ## 9 2 9 Chirodzo 2016-11-16 00:00:00 8 6 ## 10 2 10 Chirodzo 2016-12-16 00:00:00 12 23 ## # ℹ 121 more rows ## # ℹ 9 more variables: respondent_wall_type &lt;chr&gt;, rooms &lt;dbl&gt;, ## # memb_assoc &lt;chr&gt;, affect_conflicts &lt;chr&gt;, liv_count &lt;dbl&gt;, ## # items_owned &lt;chr&gt;, no_meals &lt;dbl&gt;, months_lack_food &lt;chr&gt;, instanceID &lt;chr&gt; read_csv(here(&quot;data/SAFI_clean.csv&quot;),na = &quot;NULL&quot;) %&gt;% add_number_column(4) ## # A tibble: 131 × 15 ## bestvar key_ID village interview_date no_membrs years_liv ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 1 God 2016-11-17 00:00:00 3 4 ## 2 4 2 God 2016-11-17 00:00:00 7 9 ## 3 4 3 God 2016-11-17 00:00:00 10 15 ## 4 4 4 God 2016-11-17 00:00:00 7 6 ## 5 4 5 God 2016-11-17 00:00:00 7 40 ## 6 4 6 God 2016-11-17 00:00:00 3 3 ## 7 4 7 God 2016-11-17 00:00:00 6 38 ## 8 4 8 Chirodzo 2016-11-16 00:00:00 12 70 ## 9 4 9 Chirodzo 2016-11-16 00:00:00 8 6 ## 10 4 10 Chirodzo 2016-12-16 00:00:00 12 23 ## # ℹ 121 more rows ## # ℹ 9 more variables: respondent_wall_type &lt;chr&gt;, rooms &lt;dbl&gt;, ## # memb_assoc &lt;chr&gt;, affect_conflicts &lt;chr&gt;, liv_count &lt;dbl&gt;, ## # items_owned &lt;chr&gt;, no_meals &lt;dbl&gt;, months_lack_food &lt;chr&gt;, instanceID &lt;chr&gt; Passing variables to a function The following doesn’t work, because R can’t find no_membrs, even though it does exist in the data set in the pipe. add_two_to_a_column_broken &lt;- function(data,variable) { #function that wants to add two two a column specified by the user #but it doesn&#39;t work. data %&gt;% mutate(variable = variable + 2) } read_csv(here(&quot;data/SAFI_clean.csv&quot;),na = &quot;NULL&quot;) %&gt;% add_two_to_a_column_broken(no_membrs) ## Rows: 131 Columns: 14 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (7): village, respondent_wall_type, memb_assoc, affect_conflicts, items... ## dbl (6): key_ID, no_membrs, years_liv, rooms, liv_count, no_meals ## dttm (1): interview_date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Error in `mutate()`: ## ℹ In argument: `variable = variable + 2`. ## Caused by error: ## ! object &#39;no_membrs&#39; not found For a technical note on why this is, see here. To solve this, wrap the variable in your function definition in {{ and }}, and change the = in mutatate to :=. add_two_to_a_column_fixed &lt;- function(data,variable) { #function that adds two two a column specified by the user data %&gt;% mutate( {{variable}} := {{variable}} + 2) } read_csv(here(&quot;data/SAFI_clean.csv&quot;),na = &quot;NULL&quot;) %&gt;% add_two_to_a_column_fixed(no_membrs) ## # A tibble: 131 × 14 ## key_ID village interview_date no_membrs years_liv respondent_wall_type ## &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 God 2016-11-17 00:00:00 5 4 muddaub ## 2 2 God 2016-11-17 00:00:00 9 9 muddaub ## 3 3 God 2016-11-17 00:00:00 12 15 burntbricks ## 4 4 God 2016-11-17 00:00:00 9 6 burntbricks ## 5 5 God 2016-11-17 00:00:00 9 40 burntbricks ## 6 6 God 2016-11-17 00:00:00 5 3 muddaub ## 7 7 God 2016-11-17 00:00:00 8 38 muddaub ## 8 8 Chirodzo 2016-11-16 00:00:00 14 70 burntbricks ## 9 9 Chirodzo 2016-11-16 00:00:00 10 6 burntbricks ## 10 10 Chirodzo 2016-12-16 00:00:00 14 23 burntbricks ## # ℹ 121 more rows ## # ℹ 8 more variables: rooms &lt;dbl&gt;, memb_assoc &lt;chr&gt;, affect_conflicts &lt;chr&gt;, ## # liv_count &lt;dbl&gt;, items_owned &lt;chr&gt;, no_meals &lt;dbl&gt;, months_lack_food &lt;chr&gt;, ## # instanceID &lt;chr&gt; Function to create dummies from text Remember our code to generate dummies based on text: read_csv(&quot;data/SAFI_clean.csv&quot;, na = &quot;NULL&quot;) %&gt;% mutate(items_owned = ifelse(is.na(items_owned),&quot;None&quot;,items_owned)) %&gt;% select(key_ID,items_owned) %&gt;% separate_longer_delim(items_owned, delim = &quot;;&quot;) %&gt;% mutate(value = 1) %&gt;% pivot_wider(names_from = items_owned, values_from = value, names_prefix = &quot;owns_&quot;, values_fill = 0) ## Rows: 131 Columns: 14 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (7): village, respondent_wall_type, memb_assoc, affect_conflicts, items... ## dbl (6): key_ID, no_membrs, years_liv, rooms, liv_count, no_meals ## dttm (1): interview_date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 131 × 19 ## key_ID owns_bicycle owns_television owns_solar_panel owns_table owns_cow_cart ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 1 0 ## 2 2 1 0 1 1 1 ## 3 3 0 0 0 0 0 ## 4 4 1 0 1 0 0 ## 5 5 0 0 0 0 0 ## 6 6 0 0 0 0 0 ## 7 7 0 0 0 0 0 ## 8 8 1 1 1 1 0 ## 9 9 0 1 1 0 0 ## 10 10 1 1 1 1 1 ## # ℹ 121 more rows ## # ℹ 13 more variables: owns_radio &lt;dbl&gt;, owns_cow_plough &lt;dbl&gt;, ## # owns_solar_torch &lt;dbl&gt;, owns_mobile_phone &lt;dbl&gt;, owns_motorcyle &lt;dbl&gt;, ## # owns_None &lt;dbl&gt;, owns_fridge &lt;dbl&gt;, owns_electricity &lt;dbl&gt;, ## # owns_sofa_set &lt;dbl&gt;, owns_lorry &lt;dbl&gt;, owns_sterio &lt;dbl&gt;, ## # owns_computer &lt;dbl&gt;, owns_car &lt;dbl&gt; I now want to apply this same principle to months_lack_food column. I could copy-paste the code, but the better solution is to make a function: create_dummies &lt;- function(df,stringvar,prefix=&quot;&quot;,delim = &quot;;&quot;) { #Function that generates dummies from a string variable #containing multi-response answers #returns dataset without original var, but with dummies. df %&gt;% separate_longer_delim({{stringvar}}, delim = delim) %&gt;% mutate(value = 1) %&gt;% pivot_wider(names_from = {{stringvar}}, values_from = value, names_prefix = prefix, values_fill = 0) } read_csv(here(&quot;data/SAFI_clean.csv&quot;), na = &quot;NULL&quot;) %&gt;% mutate(items_owned = ifelse(is.na(items_owned),&quot;None&quot;,items_owned)) %&gt;% create_dummies(stringvar = items_owned, prefix = &quot;owns_&quot;) %&gt;% create_dummies(months_lack_food,&quot;lack_food_&quot;) ## Rows: 131 Columns: 14 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (7): village, respondent_wall_type, memb_assoc, affect_conflicts, items... ## dbl (6): key_ID, no_membrs, years_liv, rooms, liv_count, no_meals ## dttm (1): interview_date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 131 × 43 ## key_ID village interview_date no_membrs years_liv respondent_wall_type ## &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 God 2016-11-17 00:00:00 3 4 muddaub ## 2 2 God 2016-11-17 00:00:00 7 9 muddaub ## 3 3 God 2016-11-17 00:00:00 10 15 burntbricks ## 4 4 God 2016-11-17 00:00:00 7 6 burntbricks ## 5 5 God 2016-11-17 00:00:00 7 40 burntbricks ## 6 6 God 2016-11-17 00:00:00 3 3 muddaub ## 7 7 God 2016-11-17 00:00:00 6 38 muddaub ## 8 8 Chirodzo 2016-11-16 00:00:00 12 70 burntbricks ## 9 9 Chirodzo 2016-11-16 00:00:00 8 6 burntbricks ## 10 10 Chirodzo 2016-12-16 00:00:00 12 23 burntbricks ## # ℹ 121 more rows ## # ℹ 37 more variables: rooms &lt;dbl&gt;, memb_assoc &lt;chr&gt;, affect_conflicts &lt;chr&gt;, ## # liv_count &lt;dbl&gt;, no_meals &lt;dbl&gt;, instanceID &lt;chr&gt;, owns_bicycle &lt;dbl&gt;, ## # owns_television &lt;dbl&gt;, owns_solar_panel &lt;dbl&gt;, owns_table &lt;dbl&gt;, ## # owns_cow_cart &lt;dbl&gt;, owns_radio &lt;dbl&gt;, owns_cow_plough &lt;dbl&gt;, ## # owns_solar_torch &lt;dbl&gt;, owns_mobile_phone &lt;dbl&gt;, owns_motorcyle &lt;dbl&gt;, ## # owns_None &lt;dbl&gt;, owns_fridge &lt;dbl&gt;, owns_electricity &lt;dbl&gt;, … The code is now clear, I can update the function in one place and all variables will be updated accordingly. Plus, if I want to use the same code in another project, I can easily do it. "],["data-exploration.html", "Data Exploration Setup Making a codebook from a Stata .dta Correlogram", " Data Exploration The first step of using data is exploring it. I will use Stata data because it has labels, making it easy to get a sense of the data once you get the hang of how labels are dealt with in R. Setup Download the cars data set from here or run the code below: download.file( &quot;https://raw.githubusercontent.com/kleuveld/r_cheatsheet/main/data/cars.dta&quot;, here(&quot;data/cars.dta&quot;), mode = &quot;wb&quot; ) Making a codebook from a Stata .dta In Stata variables have labels, which is great because they’re more informative than variable names. In R, it can be a bit tricky to access the labels of imported dta’s, but making a code book isn’t that hard. First, load the cars data set: library(tidyverse) library(haven) library(here) cars &lt;- read_dta(here(&quot;data/cars.dta&quot;)) The variable labels are stored as attributes of the variables. The attributes() function returns all attributes: attributes(cars$mpg) ## $label ## [1] &quot;miles per gallon&quot; ## ## $format.stata ## [1] &quot;%9.0g&quot; To see only the label use: attributes(cars$mpg)$label ## [1] &quot;miles per gallon&quot; To create a data frame with all variable labels we can apply attributes() to all variables using map_chr() from the purrr package: codebook &lt;- tibble(var = colnames(cars), label = map_chr(cars,~attributes(.x)$label)) codebook ## # A tibble: 4 × 2 ## var label ## &lt;chr&gt; &lt;chr&gt; ## 1 mpg miles per gallon ## 2 cyl number of cylinders ## 3 eng engine displacement in cubic inches ## 4 wgt vehicle weight in pounds To make it slightly more useful, we can add some summary statistics. I can apply a number of functions to a data frame using map_dbl(), which returns a named vector: list_of_functions &lt;- list(mean=mean,sd=sd,min=min,max=max) list_of_functions %&gt;% map_dbl(~.x(cars$mpg, na.rm = TRUE)) ## mean sd min max ## 23.445918 7.805007 9.000000 46.599998 To do this for every column in a dataframe, I wrap the code above in a function, and use map() to apply that function to the columns. stats_to_tibble &lt;- function(var,funs) { funs %&gt;% map_dbl(~ifelse(is.numeric(var),.x(var,na.rm = TRUE),NA)) %&gt;% as_tibble_row() } summ_stats &lt;- cars %&gt;% map(~stats_to_tibble(.x,list_of_functions)) %&gt;% list_rbind() summ_stats ## # A tibble: 4 × 4 ## mean sd min max ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 23.4 7.81 9 46.6 ## 2 5.47 1.71 3 8 ## 3 194. 105. 68 455 ## 4 2978. 849. 1613 5140 I can bind that with the codebook I had before to get a nice overview of all the variables in my dataset: bind_cols(codebook, summ_stats) ## # A tibble: 4 × 6 ## var label mean sd min max ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mpg miles per gallon 23.4 7.81 9 46.6 ## 2 cyl number of cylinders 5.47 1.71 3 8 ## 3 eng engine displacement in cubic inches 194. 105. 68 455 ## 4 wgt vehicle weight in pounds 2978. 849. 1613 5140 Here’s a re-usable function that add more columns, handles empty labels (using coalesce()) and rounds the output so it’s human-readable: create_codebook &lt;- function(.df,stats = list(mean=mean,sd=sd,min=min,max=max, prop_miss=prop_miss)) { labels &lt;- tibble(var = colnames(.df), label = map_chr(.df,function(x) coalesce(attributes(x)$label,&quot;&quot;)), type = map_chr(.df, typeof)) prop_miss &lt;- function(x,na.rm = TRUE) { mean(is.na(x)) } stats_to_tibble &lt;- function(var,stats) { map_dbl(stats,~ifelse(is.numeric(var),.x(var,na.rm = TRUE),NA)) %&gt;% as_tibble_row() } sumstats &lt;- .df %&gt;% map(~stats_to_tibble(.x,stats)) %&gt;% list_rbind() %&gt;% mutate(across(where(is.numeric), ~round(.x,2))) bind_cols(labels,sumstats) } create_codebook(cars) ## # A tibble: 4 × 8 ## var label type mean sd min max prop_miss ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mpg miles per gallon doub… 2.35e1 7.81 9 46.6 0 ## 2 cyl number of cylinders doub… 5.47e0 1.71 3 8 0 ## 3 eng engine displacement in cubic… doub… 1.94e2 105. 68 455 0 ## 4 wgt vehicle weight in pounds doub… 2.98e3 849. 1613 5140 0 Correlogram Another great data exploration tool is the correlogram, which displays the correlations between many variables. To create one, I use ggpairs() from the GGally package: library(GGally) ggpairs(cars) You can also split the correlogram by a variable, like I do with the number of cylinders below: cars %&gt;% ggpairs(columns = c(1,3,4), ggplot2::aes(colour=factor(cyl))) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
