```{r, include = FALSE}


# start with a clean environment
rm(list=ls())

# run diagnosis?
run_diagnosis <- FALSE


```


# 3. Power Analysis using DeclareDesign {#declaredesign}

There's a lot that goes into research design, 
but here we focus on one thing that can be done well in R: power calculations.
For this, I love [DeclareDesign](https://declaredesign.org/), 
a system to simulate Research Designs. 
Essentially, rather than plugging some parameters into a power calculator,
you simulate your research and so investigate the statistical properties of your design.
This is great, because it is often hard to include things like clustering and 
covariates in standard power calculators, 
and it forces you to be explicit about what you expect about your research.

This chapter will only scratch the surface of what you can do, 
so I really recommend further learning about DeclareDesign:

- [Slides by the authors of DeclareDesign: Graeme Blair, Alex Coppock, Macartan Humphreys](https://macartan.github.io/slides/202211_declaredesign_and_power.html)
- [The DeclareDesign CheatSheet](https://raw.githubusercontent.com/rstudio/cheatsheets/master/declaredesign.pdf)
-  The book [Research Design in the Social Sciences: Declaration, Diagnosis, and Redesign](https://book.declaredesign.org/)

Read at least the slides before going forward!

This chapter covers power calculations in three designs: 
a simple comparison of means, a diff-in-diff design, and a matching design.
To start of, we need example data. 
You can simulate this, but I prefer using existing data.
Here we use Rhomis data.

## Simple Power Calculation

The first design we consider is simply a comparison of means.
We are interested in the effects of a project on the input of fertilizers.


### Loading data

Here we take the Rhomis data, and add some variables. 
(For example, village codes: I create fake ones based on y-cooridnates.)


```{r}

library(tidyverse)
library(DeclareDesign)
library(here)


n_clusters = 18

rhomis <-
  read_csv(here("data/RHoMIS_Indicators.csv")) %>% #names()
  filter(Country == "Burundi") %>% #count(Livestock_Orientation)

  # split the sample in villages, by lattitude
  filter(!is.na(GPS_LAT)) %>%
  arrange(GPS_LAT) %>%
  mutate(village = rep(1:n_clusters, each = nrow(.) / n_clusters , length.out =  nrow(.))) %>%
  #select(village,HHsizeMAE,HouseholdType,Head_EducationLevel,LandOwned,HFIAS_status,NFertInput, total_income_USD_PPP_pHH_Yr) %>%
  
  # create some variables
  mutate(HFIAS_status = factor(HFIAS_status, levels = c("FoodSecure", "MildlyFI", "ModeratelyFI", "SeverelyFI")),
         food_secure = 1 * (as.numeric(HFIAS_status) <= 3),
         educated = 1*(Head_EducationLevel != "No_school"),
         female = 1* (HouseholdType == "woman_single")) %>%
  mutate(hh = row_number()) %>%
  select(!HFIAS_status) %>%

  # there are too many missings in my data; these seem reasonable to assume to be 0 when missing:
  mutate(across(c(LandOwned,NFertInput,educated),
         ~ if_else(is.na(.x),0,.x)))


```

### Declare Design 

Now it's time to start declaring our design. The first element of the 
design is the model, using `declare_model()` which essentially is my data.
`declare_model()` follows the syntax of `fabricate()`: 
a package that is part of the DeclareDesign ecosystem that allows you to generate fake data.
In this syntax, instead of using `mutate()` we can just supply new variables 
after a comma. 
In this case, I add potential outcomes, using `potential_outcomes()`:

```{r}

effect_size_simple <- 70

model_simple <- 
  declare_model(
    rhomis,
    potential_outcomes(Y ~ NFertInput + Z * effect_size_simple)
)

```

The potential outcomes are generated using `potential_outcomes()`.
This will create two variables `Y_Z_1` and `Y_Z_0`, which are 
the potential outcomes if having received treatment (`Z == 1`) or not. 
The difference between these is the effect size, which we assume to be 70 kg,
we store this in a variable, so we can play with it later on.
Note that I will generate `Z` in the next step.

Then it's time to think about assignment. 
I do a clustered randomization, based on the villages:

```{r}

assignment_simple <-
  declare_assignment(Z = cluster_ra(clusters = village, prob = 0.5)) 

```

Next, I declare my theoretical quantity of interest is the treatment 
effect in year 1. 
This is basically my research question.


```{r}

inquiry_simple <-  
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) 

```


```{r}

measurement_simple <- declare_measurement(
  Y = reveal_outcomes(Y ~ Z)
)

```

Since I won't be able to observe Y_Z_1 for the control group, nor Y_Z_0 for the treatment group,
I will need an estimation strategy.
The below arguments will be put into `lm_robust` by default, 
and the `term` argument determines which coefficient is our estimate of interest:

```{r}

estimator_simple <-
  declare_estimator(Y ~ Z,
                    clusters = village, 
                    inquiry = "ATE", 
                    term = "Z", 
                    label = "Simple")

```

Finally, I combine all these elements to declare my design. Note that 
it's only here that R starts actually running the code to randomize 
things. The previous was just declaration!

```{r}
  
design_simple <- model_simple + assignment_simple + inquiry_simple + measurement_simple + estimator_simple
summary(design_simple)

```


### Diagnosing Design and calculating power

If you want to browse a version of the data created by your design,
use the `draw_data()` function. This is useful to examine the properties of the data.

```{r}

draw_data(design_simple) %>%
  as_tibble()

```

Now to calculate our power. The `diagnose_design()` will run our
model 500 times and our power is simply the fraction of times we
find a statistically significant effect.


```{r}

set.seed(1)
diagnose_design(design_simple)

```

Our power is 0.23, meaning we found a significant result in 23% 
of our model runs.
That's way lower than the traditional 80% threshold.

How large should our effect size be to reach a power of 0.8?
We can use the `redesign()` function for this.
Redesign allows you to vary certain parameters of your design, 
and run the design 500 times for each value of the parameter.
Here I set it to run 200 times, as that takes long enough
already.


```{r diagnose_simple_power, eval = run_diagnosis}

set.seed(1)
diagnosis_simple <-
  design_simple %>%
  redesign(effect_size_simple = seq(100,300,50)) %>%
  diagnose_design(sims = 200) %>%
  tidy() %>%
  as_tibble()

```


```{r, include = FALSE, eval = run_diagnosis}

write_csv(diagnosis_simple, file = here("data/diagnosis_simple.csv"))

```

```{r,  include=FALSE}

diagnosis_simple <- read_csv(file =  here("data/diagnosis_simple.csv"))

```


Now, I want a nice plot. 
For this I filter the tibble I created, and pipe it into `ggplot()`:

```{r}

diagnosis_simple %>%
  filter(diagnosand == "power") %>%
  select(effect_size_simple,power = estimate) %>%
  ggplot(aes(x = effect_size_simple, y = power)) +
    geom_line() +
    geom_point() +
    geom_hline(yintercept=0.8,linetype=2) 

```

We can reliably detect an effect size between 150 and 200 kgs of fertilizer.
The mean fertilizer use is 70kg, so that seems unlikely to be realistic. 
Perhaps we should add covariates to reduce the standard error?

#### Adding more estimators

I define another estimator that includes covariates in
its model specification, and add it to my original design, 
hoping this will increase power.

```{r diagnose_simpe_cov_power, , eval = run_diagnosis}

estimator_simple_cov <-
  declare_estimator(Y ~ Z + LandOwned + educated + female,
                    clusters = village, 
                    inquiry = "ATE", 
                    term = "Z", 
                    label = "Simple Cov")


design_simple_cov <- design_simple + estimator_simple_cov

set.seed(1)
diagnosis_simple_cov <-
  design_simple_cov %>%
  redesign(effect_size_simple = seq(100,300,50)) %>%
  diagnose_design(sims = 200) %>%
  tidy() %>%
  as_tibble()

```



```{r, include = FALSE, eval = run_diagnosis}

write_csv(diagnosis_simple,file = here("data/diagnosis_simple.csv"))

```

```{r,  include=FALSE}

diagnosis_simple <- read_csv(file =  here("data/diagnosis_simple.csv"))

```


```{r}
diagnosis_simple_cov %>%
  filter(diagnosand == "power") %>%
  rename(power = estimate) %>%
  ggplot(aes(x = effect_size_simple, y = power, color = estimator, shape = estimator)) +
    geom_line() +
    geom_point() +
    geom_hline(yintercept=0.8,linetype=2) 

```

The power curve shifts a bit upwards, 
so that our minimum detected effect is a bit smaller, 
but the change is very small.

## Dif in diff

We can of course also opt for a diff-in-diff design. 
If fertilizer input in one year is correlated to the next, 
this should reduce standard errors and increase power.
But then we have to pretend we have two rounds of data collection.

For that, I prefer to use `dplyr`, 
but it should be possible with `fabricate()` as well.
I first rename all our time-variant varaible to have `_0` at the end, 
create a bunch of correlated varaibles with `_1` at the end, and then `pivot_longer`.


```{r}

effect_size_dd <- 70
sd_dd <- 100

model_dd <- declare_model(
    rhomis %>%
        # add one more year
        rename_with(~ paste0(., "_0"), HHsizeMAE:female) %>%
        mutate(food_secure_1 = correlate(given = food_secure_0, draw_handler = draw_binomial, prob = mean(food_secure_0) , rho = 0.9),
               HHsizeMAE_1 = HHsizeMAE_0 + rnorm(n = nrow(.), mean = 0, sd = 0.1),
               NFertInput_1 = pmax(0,NFertInput_0 + rnorm(n = nrow(.), mean = 0, sd = sd_dd))) %>%
        pivot_longer(ends_with(c("_0","_1")), 
                     names_to = c(".value", "t"),
                     names_pattern = "(.*)_([0-9]+)$",
                     values_drop_na = TRUE,
                     names_transform = list(t = as.integer)),
    potential_outcomes(Y ~ NFertInput + Z * effect_size_dd)


)

```

I modify the inquiry and assignment to only take into account `t == 1`

```{r}

inquiry_dd <-  
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0), subset = (t == 1)) 




assignment_dd <-
  declare_assignment(treatment_group = cluster_ra(clusters = village, prob = 0.5),
                     Z = t * treatment_group)



```

Measurement is unchanged:


```{r}

measurement_dd <- declare_measurement(
  Y = reveal_outcomes(Y~Z)

)

```


Then I use [`feols()`](https://lrberge.github.io/fixest/reference/feols.html)
from the [fixest](https://lrberge.github.io/fixest/index.html)
package to estimate a Two-way fixed effects (2FE) model. 
This can be easily extended to more complex 
designs ([but beware!](https://doi.org/10.1017/pan.2020.33)).

```{r}

library(fixest)
library(broom)

estimator_dd <-
  declare_estimator(Y ~ Z | treatment_group + t,
                    cluster = "village",
                    .method = feols, 
                    term = "Z", 
                    inquiry = "ATE", 
                    label = "2FE")

```

Let's have a look at our power now! 
Of course, in a DiD setting, power depends not only on the effect size, but also on how well the years correlate.
I therefore also vary the `sd_dd` variable, which adds increase between-year variation


```{r diagnose_dd_power, eval = run_diagnosis}

design_dd <- model_dd + assignment_dd + inquiry_dd + measurement_dd + estimator_dd

set.seed(1)
diagnosis_dd <-
  design_dd %>%
  redesign(effect_size_dd = seq(10,90,20),
           sd_dd = c(100,200)) %>%
  diagnose_design(sims = 100) %>%
  tidy() %>%
  as_tibble()

```


```{r, include = FALSE, eval = run_diagnosis}

write_csv(diagnosis_dd,file = here("data/diagnosis_dd.csv"))

```

```{r,  include=FALSE}

diagnosis_dd <- read_csv(file =  here("data/diagnosis_dd.csv"))

```


Putting it in a plot again:


```{r}

diagnosis_dd %>%
  filter(diagnosand == "power") %>%
  rename(power = estimate) %>%
  mutate(sd_dd = factor(sd_dd)) %>%
  ggplot(aes(x = effect_size_dd, y = power, color = sd_dd, shape = sd_dd)) +
    geom_line() +
    geom_point() +
    geom_hline(yintercept=0.8,linetype=2) 

```


Our power is now a lot better, even if we turn up the variance. 
However, this all depends on the correlation between years. 
If there is more between-year variation, you lose the benefits of a DiD design
(we could of course use `diagnose_design` to find out what the effect of between-year
variation is on power!)

## Propsensity score matching

Finally, let's imagine a Propensity Score matching scenario. We therefore need
to pretend treatment isn't random, but is correlated to the outcome. 

Let's also look at `food_secure`, since it's binary, and thus adds a twist here and there.

Finally, I will use the `resample_data()` function to pretend that we can visit 
more households than are in my original data set.

### Declare Design

There's two novel things here:

  - The outcome is binary, 
   and usually we think of effects sizes in that case as "and increase of X percentage points".
   We can just add a constant to the entire treatment group: we need to change 
   the number of food insecure households to food secure in such a way that the increase
   is a certain percentage point.
  - We don't randomly assign treatment, but rather make it correlated to the outcome. 
   This is a bit tricky, but we can use the `correlate()` function to do this. 
   We will correlate treatment with education and livestock holdings, which are both correlated with food security. 


First, let's just define a function that can flip some of the 0s in `food_secure` to 1s,
we can use that in `pontential_outcomes()` to create a certain effect size:

```{r}



x <- draw_binomial(N = 100, prob = 0.3)
Z <- draw_binomial(N = 100, prob = 0.5)
  
# this function increases the share of 1s in x[Z==1] by `effect`
flip_zeroes <- function(x, Z, effect) {
  
  # If no one is treated, return x unchanged
  if (sum(Z == 1) == 0) return(x)
  
  # how many 1s do we currently have?
  n_current_ones <- sum(x[Z == 1])

  # how many 1s should we have?
  p_current <- mean(x[Z == 1])
  p_target  <- p_current + effect
  n_target_ones  <- round(p_target * length(x[Z == 1]))
 
  # figure out which zeros to flip
  n_to_flip <- n_target_ones - n_current_ones
  
  # If nothing to flip, return x
  if (n_to_flip <= 0) return(x)
  
  zeros_index <- which(x[Z == 1] == 0)
  
  # Can't flip more zeros than exist
  n_to_flip <- min(n_to_flip, length(zeros_index))
  if (n_to_flip == 0) return(x)
  
  flip_index <- sample(zeros_index, n_to_flip)

  # do it, and return the new x
  x[Z == 1][flip_index] <- 1
  x
}




```

Our model then becomes:

```{r}

effect_size_psm = 0.3
N <- 1000

model_psm <- 
  declare_model(
    rhomis %>% 
      resample_data(N = N), 
    potential_outcomes(Y ~ flip_zeroes(food_secure, Z, effect_size_psm))
  )

```

The we do the assignment. 
We use the `correlate()` function to make treatment correlated with education and livestock holdings, which are both correlated with food security.

I made the correlation a bit complex, 
as one of the advantages of PSM is that it works even if you don't know the exact selection process, as long as you have the right covariates.

```{r}

rho <- 0.5

assignment_psm <- declare_assignment(
  Z = correlate(
      given = educated * LivestockHoldings^2, 
      rho = rho,  
      draw_binomial, 
      prob = 0.5
    )
)

```

 
Measurement and inquiry are standard. I also add an OLS estimator to check for bias.

```{r}

measurement_psm <- declare_measurement(Y = reveal_outcomes(Y ~ Z)
)

inquiry_psm <- declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)
)

estimator_notmatched_psm <- 
  declare_estimator(Y ~ Z, 
                    inquiry = "ATE", 
                    term = "Z", 
                    label = "Not matched")


```

Finally, I need to somehow put my matching model in DeclareDesign.
Fortunately, you can put any the output of any function into the `declare_estimator()` function 
using the `handler` argument. 
It does need to accept a `data` argument, 
and output a [`tidy()`](https://broom.tidymodels.org/articles/broom.html) dataframe.

My function `custom_match()` accepts a data frame, uses `MatchIt` to create
a matched data set,
I define a function that does that, and some labelling:

```{r}

# the select function would conflict with dplyr
# this syntax requires R 4.0 or higher

library(MatchIt, exclude("select"))


custom_match <- function(data,
                         equation,
                         outcome = "Y", 
                         term = "Z", 
                         method = "nearest"){

  # the command filter(term == term) wouldn't do anything, so this is a workaround
  term_value <- term

  # run the matching model
  matchit_model <- matchit(equation,
                          data = data,
                         method = method)
  matched_df <- match.data(matchit_model)

  # run a regression, weighted by the propensity score
  lm_robust(Y ~ Z, data = matched_df, weights = weights) %>%
    tidy() %>%
    mutate(outcome = outcome) %>%
    filter(term == term_value)
}

estimator_matched_psm <- declare_estimator(
  Z ~ educated + LivestockHoldings,
  handler = label_estimator(custom_match),
  inquiry = "ATE",  # Tie the estimator to the declared inquiry
  label = "Matched PSM",
  term = "Z",
)

estimator_matched_cem <- declare_estimator(
  Z ~ educated + LivestockHoldings,
  handler = label_estimator(custom_match),
  inquiry = "ATE",  # Tie the estimator to the declared inquiry
  label = "Matched CEM",
  term = "Z",
  method = "cem"
)

design_psm <-
  model_psm +  inquiry_psm + assignment_psm +  measurement_psm + estimator_notmatched_psm + estimator_matched_psm + estimator_matched_cem

```


### Checking balance

It's useful to explore the property's of our matching estimator. 
First  I draw data based on my design and create matched version of that data.

```{r}

set.seed(1)
test_data <- 
  draw_data(design_psm) %>%
  as_tibble() 


  matchit_model_psm <- matchit( Z ~ educated + LivestockHoldings,
                         data = test_data,
                         method = "nearest", 
                         discard = "both")
  test_data_psm <- match.data(matchit_model_psm)



  matchit_model_cem <- matchit( Z ~ educated + LivestockHoldings,
                         data = test_data,
                         method = "cem", 
                         discard = "both")
  test_data_cem <- match.data(matchit_model_cem)
  
```

Then I create balance table for the `test_data`:

```{r}

library(modelsummary)
library(flextable)


# balance table
test_data %>% 
  mutate(Z = factor(Z, labels = c("Control", "Treatment"))) %>%
  select(Z, Y, food_secure, HHsizeMAE, educated, LandOwned, NFertInput, female, NFertInput) %>%
  datasummary_balance( ~ Z , data = ., 
                      output = "flextable", stars = TRUE, 
                      dinm = TRUE, dinm_statistic = "p.value") %>%
  fix_border_issues() %>% 
  autofit()

```

And for the `test_data_psm`, which is slightly more balanced.

```{r}

test_data_psm %>% 
  mutate(Z = factor(Z, labels = c("Control", "Treatment"))) %>%
  select(Z, Y, food_secure, HHsizeMAE, educated, LandOwned, NFertInput, female, NFertInput) %>%
  datasummary_balance( ~ Z , data = ., 
                      output = "flextable", stars = TRUE, 
                      dinm = TRUE, dinm_statistic = "p.value") %>%
  fix_border_issues() %>% 
  autofit()

```

And for the CEM.


```{r}


test_data_cem %>% 
  mutate(Z = factor(Z, labels = c("Control", "Treatment"))) %>%
  select(Z, Y, food_secure, HHsizeMAE, educated, LandOwned, NFertInput, female, NFertInput) %>%
  datasummary_balance( ~ Z , data = ., 
                      output = "flextable", stars = TRUE, 
                      dinm = TRUE, dinm_statistic = "p.value") %>%
  fix_border_issues() %>% 
  autofit()



```

Note that while according to the [documentation](https://modelsummary.com/reference/datasummary_balance.html#global-options),
`datasummary_balance()` reports weighted means if a `weights` variable is present. 
However, there's a bug in the version of `modelsummary` used here, 
so here's a fixed balance table, using a custom function, 
with bits and pieces of code I had:


```{r}

library(flextable)
library(estimatr)

balance_table <- function(data, by){

  # get a table with summ stats:
  summstats <-
    data %>%
    select(-weights) %>%
    group_by( {{ by }} ) %>%
    summarize(
      across(
        .cols = everything(),
        .fns = list(
          mean = ~mean(.x,na.rm=TRUE),
          sd = ~sd(.x,na.rm=TRUE)
        ),
        .names =  "{.col}-{.fn}"
      )
    ) %>%
    pivot_longer(cols = -Z,
                  names_to = c("Variable",".value"),
                  names_sep="-")

  # function to get the difference between treatment and control
  get_diffs <- function(equation, 
                        clusters = NULL,
                        weights = NULL){
    reg <- 
      lm_robust(
        equation, 
        clusters = {{ clusters }},
        weights = {{ weights }}
      ) %>% 
      broom::tidy()

    estimate <- round(reg[2,2],2)

    p <- reg[2,5]

    stars = case_when(p < 0.001 ~ "***",
                      p < 0.01 ~ "**",
                      p < 0.05 ~ "*",
                      .default = "")

    paste0(estimate, stars)
    
  }

  # compute differences for all vars:

  difcol <-
    data %>%
    summarize(across(.cols = c(everything(), -Z),
                    .fns = ~get_diffs(.x ~ Z, weights = weights))) %>% 
    pivot_longer(cols =everything(),
                names_to = "Variable",
                values_to="Difference")  

  # output
  summstats %>%
    tabulator(
      rows = names(summstats)[2],
      columns = names(summstats)[1],
      datasup_last = difcol,
      Mean = as_paragraph(as_chunk(mean, digits=2)),
      SD = as_paragraph(as_chunk(sd, digits=2))
    ) %>%
    as_flextable()  %>%
    #labelizor(j = "Variable", labels = labels, part = "body") %>% 
    fix_border_issues() %>% 
    autofit() 
}


test_data_cem %>%
  select(Z, Y, food_secure, HHsizeMAE, educated, LandOwned, NFertInput, female, NFertInput, weights) %>%
  balance_table(by = Z)


```


Let's run the models two times:


```{r}

unmatched_model <- lm_robust(Y ~ Z, data = test_data)
matched_model_psm <- lm_robust(Y~Z, data = test_data_psm, weights = weights)
matched_model_cem <- lm_robust(Y~Z, data = test_data_cem, weights = weights)

modelsummary(list(lm = unmatched_model, psm = matched_model_psm, cem = matched_model_cem), output = "flextable",
             gof_map = c("nobs","r.squared","adj.r.squared"), stars = TRUE)

```

The true effect is 0.3, so CEM is clearly better able to reduce bias than PSM.

### Bias

Now, let's diagnose the design!

First, let's examine bias, given selection effects. 
The strength of the selection effect is given by `rho`,
so I use `redesign()` to vary it:

```{r diagnose_psm_bias, eval = run_diagnosis}

diagnosis_psm_bias <-
  design_psm %>%
  redesign(rho = seq(from = 0, to = 0.75, by = 0.25))  %>%
  diagnose_design(sims = 100) %>%
  tidy() %>%
  as_tibble()

```

```{r, include = FALSE, eval = run_diagnosis}

write_csv(diagnosis_psm_bias, file = here("data/diagnosis_psm_bias.csv"))

```

```{r,  include=FALSE}

diagnosis_psm_bias <- read_csv(file =  here("data/diagnosis_psm_bias.csv"))

```

And I put the result in a plot:

```{r}

diagnosis_psm_bias %>%
  filter(diagnosand == "bias") %>%
  select(rho, bias = estimate, estimator) %>%
  ggplot(aes(x = rho, y = bias, 
             shape = estimator, color=estimator)) +
    geom_line() +
    geom_point() 

```

It's clear that PSM barely affected bias, while CEM reduced it significantly.

### Power 

Then, let's examine power. 


```{r diagnose_psm_power, eval = run_diagnosis}

diagnosis_psm_power <-
  design_psm %>%
  redesign(clusters_treat = c(20,40,60),
           effect = seq(0.1,0.4,0.1)) %>%
  diagnose_design(sims = 100) %>%
  tidy() %>%
  as_tibble()

```

```{r, include = FALSE, eval = run_diagnosis}

write_csv(diagnosis_psm_power,file = here("data/diagnosis_psm_power.csv"))

```

```{r,  include=FALSE, message = FALSE}

diagnosis_psm_power <- read_csv(file =  here("data/diagnosis_psm_power.csv"))

```

When we plot the results, it seems sample size doesn't affect power much 
(not sure why, perhaps the resampling?). 
The design can pick up an `effect` of 0.4,
meaning that 40% of the food insecure beneficaries become food secure.
The actual treatment effect would be lower, as some beneficiaries would
have been food secure before the intervention.


```{r}

diagnosis_psm_power %>%
  filter(diagnosand == "power") %>%
  select(clusters_treat, power = estimate, estimator, effect) %>%
  ggplot(aes(x = clusters_treat, y = power, 
             shape = estimator, color=estimator)) +
    geom_line() +
    geom_point() +
    geom_hline(yintercept=0.8,linetype=2) +
    facet_wrap(~effect)

```

