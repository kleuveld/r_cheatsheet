# Power Anylsis using DeclareDesign


Resources for learning about Declare Design:

-  [The DeclareDesign book](https://book.declaredesign.org/)
- [The DeclareDesign CheatSheet](https://raw.githubusercontent.com/rstudio/cheatsheets/master/declaredesign.pdf)
- [Slides by Macartan Humphreys](https://macartan.github.io/slides/202211_declaredesign_and_power.html)

Read at least the slides by Macarten before going forward!


## Dif in Dif

Here's a common situation: we've collected some baseline data, and are 
wondering if our study has sufficient power to pick up expected treatment
effects.

Let's start off with some fake baseline data, which only contains `y0`, our outcome
indicator. We use the `fabricate()` function, from the fabricatr library, which is
loaded with DeclareDesign:

```{r eval=TRUE, include = TRUE, echo=TRUE, warning=TRUE, error=TRUE, message=FALSE}

library(tidyverse)
library(DeclareDesign)

N = 500

fake_data <-
  fabricate(N = N, 
            y0 = runif(N, 1, 10))

head(fake_data)

```

Now, let's add an extra year to our data, again using `fabricate()`, just
to demonstrate how:

```{r eval=TRUE, include = TRUE, echo=TRUE, warning=TRUE, error=TRUE, message=FALSE}

fakedata_years <-
  fabricate(fake_data,
            years = add_level(N = 2, t = as.numeric(years) - 1, nest=FALSE),
            observations = cross_levels(by = join_using(ID, years)),
            )

```

Now it's time to start declaring our design. Our model is quite simple, where all
households will see their `Y` increased by 10 and a small random error with standard deviation of 1, and
treatment households (indicated by `Z`) will see an additional increase of their `Y` of 20:

```{r eval=TRUE, include = TRUE, echo=TRUE, warning=TRUE, error=TRUE, message=FALSE}

year_shock <- 10
effect_size <- 20
sd <- 1

model <- 
  declare_model(fakedata_years,
                potential_outcomes(Y ~ y0 + t * year_shock + 
                                       t * Z * effect_size + 
                                       t * rnorm(N,sd = sd)))

```

Then we need to think about assignment. We need to assign treatment, and
reveal the outcomes. If treatment had already been know during baseline (not
unlikely) then we would have only had to reveal the outcomes:

```{r eval=TRUE, include = TRUE, echo=TRUE, warning=TRUE, error=TRUE, message=FALSE}

assignment <-
  declare_assignment(Z = cluster_ra(clusters = ID, prob = 0.5),
                     Y = reveal_outcomes(Y ~ Z)) 
```

Our theoretical quantity of interest is the treatment effect in year 1:

```{r eval=TRUE, include = TRUE, echo=TRUE, warning=TRUE, error=TRUE, message=FALSE}

inquiry <-  
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0), subset = (t == 1)) 

````

And finally the estimator with which we hope to compute this. We use
a standard regression model:

```{r eval=TRUE, include = TRUE, echo=TRUE, warning=TRUE, error=TRUE, message=FALSE}

estimator <-
  declare_estimator(Y ~ t * Z, inquiry = "ATE", .method = lm_robust, term = "t:Z")

```

Our design simply combines these things:

```{r eval=TRUE, include = TRUE, echo=TRUE, warning=TRUE, error=TRUE, message=FALSE}
  
  design <- model + assignment + inquiry + estimator 
  summary(design)
  
```

Now to calculate our power. The `diagnose_design()` will run our
model 500 times and our power is simply the fraction of times we
find a statistically significant effect.

```{r eval=TRUE, include = TRUE, echo=TRUE, warning=TRUE, error=TRUE, message=FALSE}

  diagnose_design(design)

```

Looks good! But what if we were too optimistic? We can vary various parameters of 
our design, and test all combinations by using the `redesign()` function. 
I've set the `sims` option of `diagnose_design()` to 50, to reduce the number
of times each variation of the model is run to save me some time.

```{r eval=TRUE, include = TRUE, echo=TRUE, warning=TRUE, error=TRUE, message=FALSE}

diagnosis <-
  design %>%
  redesign(effect_size = c(5,10,20),
           year_shock = c(5,10,20),
           sd = c(1,2,5)) %>%
  diagnose_design(sims = 50)
  

diagnosis %>%
  tidy() %>%
  filter(diagnosand == "power") %>%
  select(effect_size,year_shock,sd,power = estimate)

```

So as long as the standard deviation of our error term stays below 2, 
our power stays above 0.8.

